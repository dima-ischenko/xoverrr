СНИМОК ПРОЕКТА: /Users/isenko/GIT/xoverrr
СОЗДАН: xoverrr_lib.txt
============================================================

СТРУКТУРА ПРОЕКТА (отфильтрованная):
--------------------------------------------------

./
  src/
    xoverrr/
      adapters/
        __init__.py
        base.py
        clickhouse.py
        oracle.py
        postgres.py
      __init__.py
      constants.py
      core.py
      exceptions.py
      logger.py
      models.py
      utils.py
  tests/
    integration/
      cross_db/
        clickhouse_oracle/
          test_ch_ora_count_identical.py
          test_ch_ora_sample_discrepant.py
          test_ch_ora_sample_identical.py
        clickhouse_postgres/
          test_ch_pg_count_identical.py
          test_ch_pg_sample_identical_compound_pk.py
          test_ch_pg_sample_identical_excluded_cols.py
          test_ch_pg_sample_identical_null.py
          test_ch_pg_sample_identical_numeric.py
        oracle_postgres/
          test_ora_pg_count_identical.py
          test_ora_pg_custom_query_identical.py
          test_ora_pg_sample_identical.py
          test_ora_pg_sample_identical_boolean.py
          test_ora_pg_sample_identical_custom_pk.py
          test_ora_pg_sample_identical_dates.py
          test_ora_pg_sample_identical_null.py
          test_ora_pg_sample_identical_numeric.py
          test_ora_pg_sample_identical_timezone.py
          test_ora_pg_sample_identical_unicode_chars.py
      docker/
        clickhouse/
          init.sql
        oracle/
          init.sql
        postgres/
          init.sql
        docker-compose.yml
      self_db/
        clickhouse/
          test_clickhouse_sample.py
          test_clickhouse_sample_table_vs_table.py
        oracle/
          test_oracle_sample.py
          test_oracle_sample_data_types.py
          test_oracle_sample_identical_complex_data_types.py
        postgres/
          test_postgres_sample.py
      conftest.py
    unit/
      test_utils.py
    README.md
  project_snap.py
  pyproject.toml
  README.md

============================================================
СОДЕРЖИМОЕ ФАЙЛОВ:
============================================================


==================================================
ФАЙЛ: pyproject.toml
РАЗМЕР: 1375 символов
==================================================

# pyproject.toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "xoverrr"
version = "1.1.5"
description = "A tool for cross-database and intra-source data comparison with detailed discrepancy analysis and reporting."
readme = "README.md"
requires-python = ">=3.9"
authors = [
    {name = "Dmitry Ischenko", email = "hotmori@gmail.com"}
]
license = {text = "MIT"}
keywords = ["data-quality", "comparison", "database", "data", "quality", "engineering"]
classifiers = [
    "Intended Audience :: Developers",
    "Topic :: Database",
    "License :: OSI Approved :: MIT License",
]

dependencies = [
    "pandas>=2.0.0",
    "sqlalchemy>=2.0.0",
    "numpy>=1.24.0",
    "oracledb>=2.0.0",
    "psycopg2-binary>=2.9.0",
    "clickhouse-sqlalchemy>=0.2.0",
]

[project.urls]
Homepage = "https://github.com/dima-ischenko/xoverrr"

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "mypy>=1.0.0",
    "pre-commit>=3.0.0",
    "tenacity>=8.2.0"
]
test = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "tenacity>=8.2.0"
]
lint = [
    "black>=23.0.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
]

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
pythonpath = ["src"]
testpaths = ["tests"]
addopts = "-v"


==================================================
ФАЙЛ: README.md
РАЗМЕР: 12258 символов
==================================================

# xoverrr (pronounced “crossover”)

A tool for cross-database and intra-source data comparison with detailed discrepancy analysis and reporting.

## Key Features
- **Multi‑DBMS support**: Oracle, PostgreSQL (+ Greenplum), ClickHouse (extensible via adapter layer) — tables and views.
- **Universal connections**: Provide SQLAlchemy Engine objects for source and target databases.
- **Comparison strategies**:
  * Data sample comparison
  * Count‑based comparison with daily aggregates
  * Fully custom (raw) SQL‑query comparison
- **Smart analysis**:
  * Excludes “fresh” data to mitigate replication lag
  * Auto‑detection of primary keys and column types from DBMS metadata (PK must be found on at least one side, or may be supplied manually)
  * Application‑side type conversion
  * Automatic exclusion of columns with mismatched names
- **Optimization**: Two samples of 1 million rows × 10 columns (each ~330 MB) compared in ~3 s (Intel Core i5 / 16 GB RAM)
- **Detailed reporting**: In‑depth column‑level discrepancy analysis with example records (column view / record view)
- **Flexible configuration**: Column exclusion/inclusion, tolerance thresholds, custom primary‑key specification
- **Unit tests**: Coverage for comparison methods, functional and performance validation
- **Integrations tests**: contains integration tests for xoverrr using real databases started via Docker

## Example Report
```
================================================================================
2025-11-24 20:09:40
DATA SAMPLE COMPARISON REPORT:
public.account
VS
stage.account
================================================================================
timezone: Europe/Athens

        SELECT created_at, updated_at, id, code, bank_code, account_type, counterparty_id, special_code, case when updated_at > (now() - INTERVAL '%(exclude_recent_hours)s hours') then 'y' end as xrecently_changed
        FROM public.account
        WHERE 1=1
            AND created_at >= date_trunc('day', %(start_date)s::date)
            AND created_at < date_trunc('day', %(end_date)s::date)  + interval '1 days'

    params: {'exclude_recent_hours': 1, 'start_date': '2025-11-17', 'end_date': '2025-11-24'}
----------------------------------------

        SELECT created_at, updated_at, id, code, bank_code, account_type, counterparty_id, special_code, case when updated_at > (sysdate - :exclude_recent_hours/24) then 'y' end as xrecently_changed
        FROM stage.account
        WHERE 1=1
            AND created_at >= trunc(to_date(:start_date, 'YYYY-MM-DD'), 'dd')
            AND created_at < trunc(to_date(:end_date, 'YYYY-MM-DD'), 'dd') + 1

    params: {'exclude_recent_hours': 1, 'start_date': '2025-11-17', 'end_date': '2025-11-24'}
----------------------------------------

SUMMARY:
  Source rows: 10966
  Target rows: 10966
  Duplicated source rows: 0
  Duplicated target rows: 0
  Only source rows: 0
  Only target rows: 0
  Common rows (by primary key): 10966
  Totally matched rows: 10965
----------------------------------------
  Source only rows %: 0.00000
  Target only rows %: 0.00000
  Duplicated source rows %: 0.00000
  Duplicated target rows %: 0.00000
  Mismatched rows %: 0.00912
  Final discrepancies score: 0.00456
  Final data quality score: 99.99544
  Source-only key examples: None
  Target-only key examples: None
  Duplicated source key examples: None
  Duplicated target key examples: None
  Common attribute columns: created_at, updated_at, code, bank_code, account_type, counterparty_id, special_code
  Skipped source columns:
  Skipped target columns: mt_change_date

COLUMN DIFFERENCES:
  Discrepancies per column (max %): 0.00912
  Count of mismatches per column:

 column_name  mismatch_count
special_code               1
  Some examples:

primary_key                          column_name  source_value target_value
f8153447-****-****-****-****** special_code       N/A          XYZ

DISCREPANT DATA (first pairs):
Sorted by primary key and dataset:


created_at          updated_at          id                                   code                 bank_code account_type counterparty_id                      special_code xflg
2025-11-24 18:58:27 2025-11-24 18:58:27 f8153447-****-****-****-****** 42****************87 0********* 11           62aa01a6-****-****-****-f17e2b*****4
N/A       src
2025-11-24 18:58:27 2025-11-24 18:58:27 f8153447-****-****-****-****** 42****************87 0********* 11           62aa01a6-****-****-****-f17e2b*****4 XYZ       trg

================================================================================
```

## Metric Calculation
### for compare_sample/compare_custom_query
```
final_diff_score =
 (source_dup% × 0.1)
 + (target_dup% × 0.1)
 + (source_only_rows% × 0.15)
 + (target_only_rows% × 0.15)
 + (rows_mismatched_by_any_column% × 0.5)
```

### for compare_counts
```
sum_of_absolute_differences = `abs(source_count - target_count)` per each day
sum_of_common_counts = `min(source_count, target_count)` per each day
final_diff_score = 100 × (sum_of_absolute_differences) / (sum_of_absolute_differences + sum_of_common_counts)
```

#### Quality score formula all methods: `100 − final_diff_score`
#### Scores range 0–100%; higher values indicate better data quality.

## Comparison Methods

### 1. Data Sample Comparison (`compare_sample`)
Suitable for comparing row sets and column values over a date range.

```python
status, report, stats, details = comparator.compare_sample(
    source_table=DataReference("table_name", "schema_name"),
    target_table=DataReference("table_name", "schema_name"),
    date_column="created_at",
    update_column="modified_date",
    date_range=("2024-01-01", "2024-01-31"),
    exclude_columns=["audit_timestamp", "internal_id"],
    include_columns=None,
    custom_primary_key=["id", "user_id"],
    tolerance_percentage=1.0,
    exclude_recent_hours=24,
    max_examples=3
)
```

**Parameters:**
- `source_table`, `target_table` – names of the tables or views to compare
- `date_column` – column used for date‑range filtering
- `update_column` – column identifying “fresh” data (excluded from both sides)
- `date_range` – tuple `(start_date, end_date)` in “YYYY‑MM‑DD” format
- `exclude_columns` – list of columns to omit from comparison, aka blacklist
- `include_columns` – list of columns to include, aka whitelist
- `custom_primary_key` – user‑specified primary key (if not provided, auto‑detected)
- `tolerance_percentage` – acceptable discrepancy threshold (0.0–100.0)
- `exclude_recent_hours` – exclude data modified within the last N hours
- `max_examples` – maximum number of discrepancy examples included in the report

### 2. Count‑Based Comparison (`compare_counts`)
Efficient for large‑volume comparisons over extended date ranges, identifying missing rows or duplicates.

```python
status, report, stats, details = comparator.compare_counts(
    source_table=DataReference("users", "schema1"),
    target_table=DataReference("users", "schema2"),
    date_column="created_at",
    date_range=("2024-01-01", "2024-01-31"),
    tolerance_percentage=2.0,
    max_examples=5
)
```

**Parameters:**
- `source_table`, `target_table` – references to the tables/views to compare
- `date_column` – column for daily grouping
- `date_range` – date interval for analysis
- `tolerance_percentage` – acceptable discrepancy threshold
- `max_examples` – maximum number of daily discrepancy examples included in the report

### 3. Custom‑Query Comparison (`compare_custom_query`)
Compares data from arbitrary SQL queries. Suitable for complex scenarios.

```python
status, report, stats, details = comparator.compare_custom_query(
    source_query="""SELECT id as user_id, name as user_name, created_at as created_date FROM scott.source_table WHERE status = %(status)s""",
    source_params={'status': 'active'},
    target_query="""SELECT user_id, user_name, created_date FROM scott.target_table WHERE status = :status""",
    target_params={'status': 'active'},
    custom_primary_key=["id"],
    exclude_columns=["internal_code"],
    tolerance_percentage=0.5,
    max_examples=3
)
```

**Parameters:**
- `source_query`, `target_query` – parameterised SQL queries for the source and target
- `source_params`, `target_params` – query parameters
- `custom_primary_key` – mandatory list of column names constituting the primary key
- `exclude_columns` – columns to omit from comparison
- `tolerance_percentage` – acceptable discrepancy threshold
- `max_examples` – maximum number of discrepancy examples included in the report
- To automatically exclude recently changed records, add the following expression to your SELECT clause in `compare_custom_query`:
  ```sql
  case when updated_at > (sysdate - 3/24) then 'y' end as xrecently_changed
  ```

**Automatic Primary‑Key Detection:**
- If `custom_primary_key` is not supplied, the system automatically infers the PK from metadata.
- When source and target PKs differ, the source PK is used with a warning.

**Performance Considerations:**
- DataFrame size validation (hard limit: 3 GB per sample)
- Efficient comparison via XOR properties
- Configurable limits via constants

**Return Values:**
All methods return a tuple:
- `status` – comparison status (`COMPARISON_SUCCESS` / `COMPARISON_FAILED` / `COMPARISON_SKIPPED`)
- `report` – textual report detailing discrepancies
- `stats` – `ComparisonStats` dataclass instance containing comparison statistics
- `details` – `ComparisonDiffDetails` dataclass instance with discrepancy examples and details

### Status Types
- **COMPARISON_SUCCESS**: Comparison completed within tolerance limits.
- **COMPARISON_FAILED**: Discrepancies exceed tolerance threshold, or a technical error occurred.
- **COMPARISON_SKIPPED**: No data available for comparison (both tables empty).

### Structured Logging
Logs include timing information and structured context:
```
2024-01-15 10:30:45 - INFO - xoverrr.core._compare_samples - Query executed in 2.34s
2024-01-15 10:30:46 - INFO - xoverrr.core._compare_samples - Source: 150000 rows, Target: 149950 rows
2024-01-15 10:30:47 - INFO - xoverrr.utils.compare_dataframes - Comparison completed in 1.2s
```

### Tolerance Percentage
- **tolerance_percentage**: Acceptable discrepancy threshold (0.0–100.0).
- If `final_diff_score > tolerance`: status = `COMPARISON_FAILED`
- If `final_diff_score ≤ tolerance`: status = `COMPARISON_SUCCESS`
- Enables configuration of acceptable discrepancy levels.

---

## Usage Example
**Sample comparison** (Greenplum vs Oracle):

```python
from xoverrr import DataQualityComparator, DataReference, COMPARISON_SUCCESS, COMPARISON_FAILED, COMPARISON_SKIPPED
import os
from datetime import date, timedelta

USER_ORA = os.getenv('USER_ORA', '')
PASSWORD_ORA = os.getenv('PASSWORD_ORA', '')

USER_GP = os.getenv('USER_GP', '')
PASSWORD_GP = os.getenv('PASSWORD_GP', '')

HOST = os.getenv('HOST', '')

def create_src_engine(user, password, host):
    """Source engine (Oracle)"""
    os.environ['NLS_LANG'] = '.AL32UTF8'
    return create_engine(f'oracle+oracledb://{user}:{password}@{host}:1521/?service_name=dwh')

def create_trg_engine(user, password, host):
    """Target engine (Postgres/Greenplum)"""
    connection_string = f'postgresql+psycopg2://{user}:{password}@{host}:5432/adb'
    engine = create_engine(connection_string)
    return engine



src_engine = create_src_engine(USER_ORA, PASSWORD_ORA, HOST)
trg_engine = create_trg_engine(USER_GP, PASSWORD_GP, HOST)

comparator = DataQualityComparator(
    source_engine=src_engine,
    target_engine=trg_engine,
    timezone='Asia/Yekaterinburg'
)

source = DataReference("users", "schema1")
target = DataReference("users", "schema2")

FORMAT = '%Y-%m-%d'
recent_range_end = date.today()
recent_range_begin = recent_range_end - timedelta(days=1)

status, report, stats, details = comparator.compare_sample(
    source,
    target,
    date_column="created_at",
    update_column="modified_date",
    exclude_columns=["audit_timestamp", "internal_id"],
    exclude_recent_hours=24,
    date_range=(
        recent_range_begin.strftime(FORMAT),
        recent_range_end.strftime(FORMAT)
    ),
    tolerance_percentage=0
)

print(report)
if status == COMPARISON_FAILED:
    raise Exception("Sample check failed")
```

---




==================================================
ФАЙЛ: project_snap.py
РАЗМЕР: 15632 символов
==================================================

import os
import re
import fnmatch
from pathlib import Path
from typing import Set, List, Pattern, Optional, Dict
import argparse

class GitIgnorePatterns:
    """Класс для обработки шаблонов в стиле .gitignore"""

    @staticmethod
    def pattern_to_regex(pattern: str) -> Pattern:
        """
        Конвертирует .gitignore-like паттерн в регулярное выражение
        """
        # Убираем пробелы в начале/конце
        pattern = pattern.strip()

        # Игнорируем комментарии и пустые строки
        if not pattern or pattern.startswith('#'):
            return re.compile(r'(?!)')  # Ничего не матчит

        # Экранируем специальные символы regex, кроме *
        pattern = re.escape(pattern)

        # Заменяем экранированные * на .*
        pattern = pattern.replace(r'\*', '.*')

        # Обработка паттернов начинающихся с /
        if pattern.startswith(r'\/'):
            pattern = pattern[2:]  # Убираем \/
        else:
            pattern = '.*' + pattern  # Матчит в любой директории

        # Обработка паттернов заканчивающихся на /
        if pattern.endswith(r'\/'):
            pattern = pattern[:-2] + '(/.*)?'

        # Добавляем якоря
        pattern = f'^{pattern}$'

        return re.compile(pattern)

    @staticmethod
    def should_ignore(path: Path, patterns: List[str], base_dir: Path) -> bool:
        """
        Проверяет, должен ли путь быть проигнорирован на основе шаблонов
        """
        relative_path = str(path.relative_to(base_dir))

        for pattern_str in patterns:
            pattern_regex = GitIgnorePatterns.pattern_to_regex(pattern_str)
            if pattern_regex.match(relative_path):
                return True

        return False

class ProjectSnapshot:
    def __init__(self, root_dir: str, output_file: str):
        self.root_dir = Path(root_dir).resolve()
        self.output_file = output_file

        # Дефолтные настройки
        self.default_whitelist = ['*.sql', '*.py', '*.js', '*.html', '*.md', '*.json', '*.yml', '*.toml']
        self.default_blacklist = ['python3.13','__pycache__', '.pytest_cache', '.git', 'node_modules', '.venv', '*.pyc']

    def should_include_file(self, file_path: Path,
                          whitelist: Optional[List[str]] = None,
                          blacklist: Optional[List[str]] = None) -> bool:
        """
        Определяет, нужно ли включать файл на основе whitelist/blacklist
        """
        if whitelist is None:
            whitelist = self.default_whitelist
        if blacklist is None:
            blacklist = self.default_blacklist

        relative_path = str(file_path.relative_to(self.root_dir))

        # Сначала проверяем blacklist
        for pattern in blacklist:
            if fnmatch.fnmatch(relative_path, pattern) or fnmatch.fnmatch(file_path.name, pattern):
                return False

        # Если whitelist пустой, включаем все (кроме blacklist)
        if not whitelist:
            return True

        # Проверяем whitelist
        for pattern in whitelist:
            if fnmatch.fnmatch(relative_path, pattern) or fnmatch.fnmatch(file_path.name, pattern):
                return True

        return False

    def should_include_dir(self, dir_path: Path,
                         blacklist: Optional[List[str]] = None) -> bool:
        """
        Определяет, нужно ли включать директорию
        """
        if blacklist is None:
            blacklist = self.default_blacklist

        relative_path = str(dir_path.relative_to(self.root_dir))

        for pattern in blacklist:
            if fnmatch.fnmatch(relative_path, pattern) or fnmatch.fnmatch(dir_path.name, pattern):
                return False

        return True

    def load_patterns_from_file(self, file_path: str) -> List[str]:
        """Загружает паттерны из файла (как .gitignore)"""
        patterns = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        patterns.append(line)
        except FileNotFoundError:
            pass
        return patterns

    def get_filtered_structure(self, whitelist: List[str], blacklist: List[str]) -> Dict[Path, List[Path]]:
        """
        Возвращает отфильтрованную структуру проекта: {директория: [файлы]}
        """
        structure = {}

        for root, dirs, files in os.walk(self.root_dir):
            root_path = Path(root)

            # Фильтруем директории для обхода
            dirs[:] = [d for d in dirs
                      if self.should_include_dir(root_path / d, blacklist)]

            # Получаем файлы, которые прошли фильтрацию
            included_files = []
            for file in files:
                file_path = root_path / file
                if self.should_include_file(file_path, whitelist, blacklist):
                    included_files.append(file_path)

            # Добавляем директорию в структуру только если в ней есть файлы
            if included_files:
                structure[root_path] = included_files

        return structure

    def has_files_in_subtree(self, dir_path: Path, whitelist: List[str], blacklist: List[str]) -> bool:
        """
        Проверяет, есть ли в поддереве директории файлы, которые прошли фильтрацию
        """
        for root, dirs, files in os.walk(dir_path):
            root_path = Path(root)

            # Фильтруем директории для обхода
            dirs[:] = [d for d in dirs
                      if self.should_include_dir(root_path / d, blacklist)]

            for file in files:
                file_path = root_path / file
                if self.should_include_file(file_path, whitelist, blacklist):
                    return True

        return False

    def get_directory_tree(self, whitelist: List[str], blacklist: List[str]) -> List[Path]:
        """
        Возвращает список директорий, которые содержат файлы или поддиректории с файлами
        """
        relevant_dirs = set()

        # Сначала находим все директории с файлами
        structure = self.get_filtered_structure(whitelist, blacklist)
        for dir_path in structure.keys():
            # Добавляем все родительские директории, включая корневую
            current = dir_path
            while current != self.root_dir.parent:  # Изменено условие
                relevant_dirs.add(current)
                current = current.parent

        # Добавляем корневую директорию, если в ней есть файлы
        if self.root_dir in structure:
            relevant_dirs.add(self.root_dir)

        return sorted(relevant_dirs, key=lambda x: len(x.parts))

    def create_snapshot(self,
                       whitelist: Optional[List[str]] = None,
                       blacklist: Optional[List[str]] = None,
                       whitelist_file: Optional[str] = None,
                       blacklist_file: Optional[str] = None):
        """
        Создает снимок проекта с поддержкой whitelist/blacklist
        """
        # Загружаем паттерны из файлов если указаны
        final_whitelist = whitelist or self.default_whitelist.copy()
        final_blacklist = blacklist or self.default_blacklist.copy()

        if whitelist_file:
            final_whitelist.extend(self.load_patterns_from_file(whitelist_file))

        if blacklist_file:
            final_blacklist.extend(self.load_patterns_from_file(blacklist_file))

        with open(self.output_file, 'w', encoding='utf-8') as f:
            self._write_header(f)
            self._write_structure(f, final_whitelist, final_blacklist)
            self._write_contents(f, final_whitelist, final_blacklist)

    def _write_header(self, file_obj):
        """Записывает заголовок файла"""
        file_obj.write(f"СНИМОК ПРОЕКТА: {self.root_dir}\n")
        file_obj.write(f"СОЗДАН: {os.path.basename(self.output_file)}\n")
        file_obj.write("=" * 60 + "\n\n")

    def _write_structure(self, file_obj, whitelist, blacklist):
        """Записывает структуру проекта с учетом фильтрации"""
        file_obj.write("СТРУКТУРА ПРОЕКТА (отфильтрованная):\n")
        file_obj.write("-" * 50 + "\n\n")

        # Получаем структуру файлов для быстрой проверки
        structure = self.get_filtered_structure(whitelist, blacklist)
        files_by_dir = {dir: [f.name for f in files] for dir, files in structure.items()}
        
        # Рекурсивная функция для обхода директорий
        def walk_directory(current_dir, level=0):
            indent = '  ' * level
            
            # Отображаем текущую директорию
            if current_dir == self.root_dir:
                file_obj.write(f"./\n")
            else:
                file_obj.write(f"{indent}{current_dir.name}/\n")
            
            # Получаем содержимое директории
            items = []
            try:
                for item in current_dir.iterdir():
                    # Пропускаем скрытые файлы/папки, начинающиеся с точки
                    if item.name.startswith('.'):
                        continue
                        
                    # Проверяем директории
                    if item.is_dir():
                        if self.should_include_dir(item, blacklist):
                            # Проверяем, есть ли в этой директории или ее поддиректориях файлы
                            if self.has_files_in_subtree(item, whitelist, blacklist):
                                items.append((item, 'dir'))
                    # Проверяем файлы
                    elif item.is_file():
                        if self.should_include_file(item, whitelist, blacklist):
                            items.append((item, 'file'))
            except (PermissionError, OSError):
                return
            
            # Сортируем: сначала директории, потом файлы
            dirs = [item for item in items if item[1] == 'dir']
            files = [item for item in items if item[1] == 'file']
            dirs.sort(key=lambda x: x[0].name.lower())
            files.sort(key=lambda x: x[0].name.lower())
            sorted_items = dirs + files
            
            # Обрабатываем элементы
            for item, item_type in sorted_items:
                if item_type == 'dir':
                    walk_directory(item, level + 1)
                else:
                    subindent = '  ' * (level + 1)
                    file_obj.write(f"{subindent}{item.name}\n")
        
        # Начинаем обход с корневой директории
        walk_directory(self.root_dir)

    def _write_structure_old(self, file_obj, whitelist, blacklist):
        """Записывает структуру проекта с учетом фильтрации"""
        file_obj.write("СТРУКТУРА ПРОЕКТА (отфильтрованная):\n")
        file_obj.write("-" * 50 + "\n\n")

        # Получаем все релевантные директории
        relevant_dirs = self.get_directory_tree(whitelist, blacklist)

        # Сортируем директории по уровню вложенности
        relevant_dirs.sort(key=lambda x: len(x.relative_to(self.root_dir).parts))

        # Создаем словарь для быстрого доступа к файлам в директориях
        structure = self.get_filtered_structure(whitelist, blacklist)

        for dir_path in relevant_dirs:
            level = len(dir_path.relative_to(self.root_dir).parts)
            indent = '  ' * level

            # Отображаем директорию
            file_obj.write(f"{indent}{dir_path.name}/\n")

            # Отображаем файлы в этой директории, если они есть
            if dir_path in structure:
                subindent = '  ' * (level + 1)
                for file_path in structure[dir_path]:
                    file_obj.write(f"{subindent}{file_path.name}\n")

    def _write_contents(self, file_obj, whitelist, blacklist):
        """Записывает содержимое файлов"""
        file_obj.write("\n" + "=" * 60 + "\n")
        file_obj.write("СОДЕРЖИМОЕ ФАЙЛОВ:\n")
        file_obj.write("=" * 60 + "\n\n")

        structure = self.get_filtered_structure(whitelist, blacklist)

        for dir_path, files in structure.items():
            for file_path in files:
                try:
                    content = file_path.read_text(encoding='utf-8')
                    relative_path = file_path.relative_to(self.root_dir)

                    file_obj.write(f"\n{'='*50}\n")
                    file_obj.write(f"ФАЙЛ: {relative_path}\n")
                    file_obj.write(f"РАЗМЕР: {len(content)} символов\n")
                    file_obj.write(f"{'='*50}\n\n")
                    file_obj.write(content)
                    file_obj.write("\n\n")

                except UnicodeDecodeError:
                    relative_path = file_path.relative_to(self.root_dir)
                    file_obj.write(f"\n[БИНАРНЫЙ ФАЙЛ: {relative_path}]\n\n")
                except Exception as e:
                    relative_path = file_path.relative_to(self.root_dir)
                    file_obj.write(f"\n[ОШИБКА ЧТЕНИЯ {relative_path}: {e}]\n\n")

def main():
    parser = argparse.ArgumentParser(
        description='Создание структуры проекта и объединение файлов с поддержкой whitelist/blacklist'
    )
    parser.add_argument('directory', help='Корневая директория проекта')
    parser.add_argument('-o', '--output', default='project_snapshot.txt',
                       help='Выходной файл (по умолчанию: project_snapshot.txt)')

    # Whitelist/Blacklist аргументы
    parser.add_argument('-w', '--whitelist', nargs='+',
                       help='Паттерны для включения файлов (например: "*.py" "src/*" "*.json")')
    parser.add_argument('-b', '--blacklist', nargs='+',
                       help='Паттерны для исключения файлов/директорий')

    # Файлы с паттернами
    parser.add_argument('--whitelist-file',
                       help='Файл с паттернами whitelist (в стиле .gitignore)')
    parser.add_argument('--blacklist-file',
                       help='Файл с паттернами blacklist (в стиле .gitignore)')

    # Дополнительные опции
    parser.add_argument('--no-defaults', action='store_true',
                       help='Не использовать дефолтные whitelist/blacklist')
    parser.add_argument('--show-empty-dirs', action='store_true',
                       help='Показывать пустые директории в структуре')

    args = parser.parse_args()

    snapshot = ProjectSnapshot(args.directory, args.output)

    if args.no_defaults:
        snapshot.default_whitelist = []
        snapshot.default_blacklist = []

    # Переопределяем метод для показа пустых директорий если нужно
    if args.show_empty_dirs:
        original_get_directory_tree = snapshot.get_directory_tree
        def get_directory_tree_with_empty(whitelist, blacklist):
            # Включаем все директории, которые не в blacklist
            all_dirs = set()
            for root, dirs, _ in os.walk(snapshot.root_dir):
                root_path = Path(root)
                dirs[:] = [d for d in dirs if snapshot.should_include_dir(root_path / d, blacklist)]
                for d in dirs:
                    all_dirs.add(root_path / d)
            # Добавляем корневую директорию
            all_dirs.add(snapshot.root_dir)
            return sorted(all_dirs, key=lambda x: len(x.parts))

        snapshot.get_directory_tree = get_directory_tree_with_empty

    snapshot.create_snapshot(
        whitelist=args.whitelist,
        blacklist=args.blacklist,
        whitelist_file=args.whitelist_file,
        blacklist_file=args.blacklist_file
    )

    print(f"Снимок проекта сохранен в: {args.output}")
    print(f"Размер: {os.path.getsize(args.output)} байт")

if __name__ == "__main__":
    main()


==================================================
ФАЙЛ: tests/README.md
РАЗМЕР: 4574 символов
==================================================

# Unit Tests

## Installation & Setup

### Create and Activate Virtual Environment

```bash
# Create virtual environment in project root
python3 -m venv venv

# Activate it (Linux/macOS)
source venv/bin/activate

# Check Python version (should be 3.9+)
python --version
```

### Install Dependencies

```bash
# Navigate to the root directory
# Install package in development mode with test dependencies
pip install -e ".[dev,test]"

# If pip command is not found, use pip3
# pip3 install -e ".[dev,test]"

# Or install all dependencies separately
pip install -e .
pip install pytest pytest-cov
```

### Virtual Environment Issues
If package imports fail:
```bash
# Ensure you're in the project root
cd /path/to/xoverrr

# Reactivate virtual environment
deactivate
source venv/bin/activate

# Reinstall in development mode
pip install -e ".[dev,test]"
```

### Python Command Issues
If `python` command is not found:
```bash
# Use python3 explicitly
python3 -m venv venv
source venv/bin/activate
python3 -m pip install -e ".[dev,test]"
python3 -m pytest tests -v
```

```bash
# Remove virtual environment (when done)
deactivate
rm -rf venv/
```

## Running unit Tests

From the project root directory (with virtual environment activated):

```bash
# Run all integration tests
python -m pytest tests/unit -v

# Run specific test file
python -m pytest tests/unit/test_utils.py -v

# Run specific test method
pytest tests/unit/test_utils.py::TestUtils::test_duplicate_primary_keys_in_target -v 

# Run with coverage report
python -m pytest tests/unit --cov=src --cov-report=html -v
```


# Integration Tests (CI)

This sections contains integration tests for xoverrr using real databases started via Docker.

## Prerequisites

- **Linux/macOS environment**
- **Python 3.9+** with `venv` module available
- **Docker**
- **Docker Compose**

### macOS-specific Requirements (Apple Silicon M1/M2/M3 processors)
For macOS with Apple Silicon (M1/M2/M3) processors, Oracle database requires x86_64 architecture. Use Colima to run x86_64 containers:

```bash
# Install required tools via Homebrew
brew install colima docker docker-compose

# Start Colima with x86_64 architecture
colima start --arch x86_64 --memory 8 --cpu 4 --disk 40 --vm-type=vz --mount-type=sshfs


# Set Docker context to Colima
docker context use colima
```

**Note for Linux/Intel Mac users:** Colima is not required. You can use Docker Desktop or native Docker directly.


### 3. Start Test Databases

```bash
# Navigate to the integration directory
cd tests/integration

# Start all databases in detached mode
docker-compose -f docker/docker-compose.yml up -d

# Wait for databases to be ready (healthchecks will complete)
# Check status
docker-compose -f docker/docker-compose.yml ps
```

### 4. Reset Databases (Clean State)

```bash
# Stop and remove containers with volumes
docker-compose -f docker/docker-compose.yml down -v

# Restart fresh
docker-compose -f docker/docker-compose.yml up -d
```

## Running integration Tests

From the project root directory (with virtual environment activated):

```bash
# Run all integration tests
python -m pytest tests/integration -v

# Run specific test file
python -m pytest tests/integration/data_types/test_data_types.py -v

# Run specific test method
python -m pytest tests/integration/test_edge_cases.py::TestCustomQueryComparison::test_custom_query_comparison -v
```

## Test Database Credentials

The test containers use the following credentials:

| Database   | Host      | Port | User     | Password | Database | Schema |
|------------|-----------|------|----------|----------|----------|----------|
| PostgreSQL | localhost | 5433 | test_user| test_pass | test_db  | test     |
| Oracle     | localhost | 1521 | test     | test_pass | test_db  | test     |
| ClickHouse | localhost | 8123 | test_user| test_pass | test     | test     |

## Troubleshooting

### Oracle Connection Issues on macOS
If Oracle tests fail on Apple Silicon Mac:
1. Ensure Colima is running with x86_64 architecture
2. Verify Docker context is set to Colima: `docker context use colima`
3. Restart containers: `docker-compose -f docker/docker-compose.yml down -v && docker-compose -f docker/docker-compose.yml up -d`


```bash
# Run with coverage report
python -m pytest tests --cov=src --cov-report=html -v
```

### Database Health Checks
Check if databases are healthy:
```bash
docker-compose -f docker/docker-compose.yml ps

# Check individual container logs
docker-compose -f docker/docker-compose.yml logs postgres
docker-compose -f docker/docker-compose.yml logs oracle
```



==================================================
ФАЙЛ: tests/unit/test_utils.py
РАЗМЕР: 19894 символов
==================================================

import pytest
import pandas as pd
import numpy as np
import time

from xoverrr.utils import   (
    compare_dataframes,
    prepare_dataframe,
    cross_fill_missing_dates,
    ComparisonStats,
    ComparisonDiffDetails,
    validate_dataframe_size,
    get_dataframe_size_gb,
    clean_recently_changed_data,
    generate_comparison_sample_report
)


class TestUtils:
    """Unit tests for utility functions"""
    
    def test_prepare_dataframe_basic(self):
        """Test basic dataframe preparation with null handling"""
        df = pd.DataFrame({
            'col1': [1, 2, np.nan, 4],
            'col2': ['a', ' ', None, 'd'],
            'col3': [1.0, 2.5, 3.0, 4.0]
        })
        
        result = prepare_dataframe(df)
        
        assert result.shape == df.shape
        assert result['col1'].iloc[2] == 'N/A'
        assert result['col2'].iloc[1] == 'N/A'
        assert result['col2'].iloc[2] == 'N/A'
    
    def test_compare_dataframes_identical(self):
        """Test comparison of identical dataframes"""
        df1 = pd.DataFrame({
            'id': [1, 2, 3],
            'name': ['Alice', 'Bob', 'Charlie'],
            'age': [25, 30, 35]
        })
        
        df2 = pd.DataFrame({
            'id': [1, 2, 3],
            'name': ['Alice', 'Bob', 'Charlie'],
            'age': [25, 30, 35]
        })
        
        stats, details = compare_dataframes(df1, df2, ['id'], 3)
        
        assert stats.total_source_rows == 3
        assert stats.total_target_rows == 3
        assert stats.common_pk_rows == 3
        assert stats.total_matched_rows == 3
        assert stats.final_diff_score == pytest.approx(0.0, rel=1e-5)
    
    @pytest.mark.parametrize("df1_ids, df2_ids, expected_score", [
        ([1, 2, 3], [1, 2, 3], 0.0),  # Идентичные
        ([1, 2, 3], [1, 2, 4], 15.0), # Разные ключи
        ([1, 2, 3], [1, 2, 3], 0.0),  # Идентичные значения
    ])
    def test_compare_dataframes_parametrized(self, df1_ids, df2_ids, expected_score):
        """Parametrized test for dataframe comparison"""
        df1 = pd.DataFrame({
            'id': df1_ids,
            'value': ['A', 'B', 'C']
        })
        
        df2 = pd.DataFrame({
            'id': df2_ids,
            'value': ['A', 'B', 'C' if df1_ids == df2_ids else 'X']
        })
        
        stats, details = compare_dataframes(df1, df2, ['id'])
        assert stats.final_diff_score == pytest.approx(expected_score, rel=1e-5)
    
    def test_compare_dataframes_different_values(self):
        """Test comparison with different values"""
        df1 = pd.DataFrame({
            'id': [1, 2, 3],
            'name': ['Alice', 'Bob', 'Charlie'],
            'age': [25, 30, 35]
        })
        
        df2 = pd.DataFrame({
            'id': [1, 2, 3],
            'name': ['Alice', 'Robert', 'Charlie'],
            'age': [25, 31, 36]
        })
        
        stats, details = compare_dataframes(df1, df2, ['id'], 3)
        
        assert stats.common_pk_rows == 3
        expected_score = (2/3) * 100 * 0.5
        assert stats.final_diff_score == pytest.approx(expected_score, rel=1e-5)
        assert len(details.discrepancies_per_col_examples) == 3
    
    def test_compare_dataframes_empty(self):
        """Test comparison with empty dataframes"""
        df1 = pd.DataFrame({'id': [], 'name': []})
        df2 = pd.DataFrame({'id': [], 'name': []})
        
        stats, details = compare_dataframes(df1, df2, ['id'], 3)
        assert stats is None
        assert details is None
    
    def test_compare_dataframes_missing_columns_raises(self):
        """Test comparison with missing key columns raises error"""
        df1 = pd.DataFrame({'id': [1], 'name': ['Alice']})
        df2 = pd.DataFrame({'name': ['Alice']})  # Missing id column
        
        with pytest.raises(ValueError, match="Key columns missing in target"):
            compare_dataframes(df1, df2, ['id'], 3)
    
    def test_cross_fill_missing_dates(self):
        """Test cross-filling missing dates"""
        df1 = pd.DataFrame({
            'dt': pd.to_datetime(['2023-01-01', '2023-01-02']),
            'cnt': [10, 20]
        })
        
        df2 = pd.DataFrame({
            'dt': pd.to_datetime(['2023-01-02', '2023-01-03']),
            'cnt': [15, 25]
        })
        
        result1, result2 = cross_fill_missing_dates(df1, df2)
        
        assert len(result1) == 3
        assert len(result2) == 3
        assert result1['cnt'].sum() == 30
        assert result2['cnt'].sum() == 40
    
    def test_performance_small_dataframe(self):
        """Performance test for small dataframes"""
        n_records = 10000
        
        df1 = pd.DataFrame({
            'id': range(n_records),
            'value': [f'text_{i}' for i in range(n_records)],
            'value2': [f'text_{i}' for i in range(n_records)],
            'value3': [f'text_{i}' for i in range(n_records)],
            'value4': [f'text_{i}' for i in range(n_records)],
            'value5': [f'text_{i}' for i in range(n_records)],
            'value6': [f'text_{i}' for i in range(n_records)],
            'value7': [f'text_{i}' for i in range(n_records)],
            'value8': [f'text_{i}' for i in range(n_records)],
            'value9': [f'text_{i}' for i in range(n_records)],
            'value10': [f'text_{i}' for i in range(n_records)],
        })
        
        df2 = df1.copy()
        df2.loc[10:15, 'value'] = 'modified'
        
        start_time = time.time()
        stats, details = compare_dataframes(df1, df2, ['id'])
        execution_time = time.time() - start_time
        
        assert execution_time < 1.0  # Should complete in <1 second
        assert stats.final_diff_score == pytest.approx(0.03, rel=1e-5)
    
    def test_performance_medium_dataframe(self):
        """Performance test for medium dataframes"""
        n_records = 1000 * 1000
        
        df1 = pd.DataFrame({
            'id': range(1, n_records + 1),
            'int_col': 1,
            'float_col': np.random.rand(n_records),
            'str_col': [f'text_{i}' for i in range(n_records)],
            'str_col2': [f'text_a_{i}' for i in range(n_records)],
            'str_col3': [f'text_b_{i}' for i in range(n_records)],
            'str_col4': [f'text_c_{i}' for i in range(n_records)],
            'str_col5': [f'text_d_{i}' for i in range(n_records)],
            'str_col6': [f'text_d_{i}' for i in range(n_records)],
            'bool_col': np.random.choice([True, False], n_records)
        })
        
        df2 = df1.copy()
        
        # Change few records
        k = 100
        change_indices = np.random.choice(np.arange(1, n_records), size=k, replace=False)
        for idx in change_indices:
            df2.loc[idx, 'float_col'] += 0.1
        df2.loc[change_indices[0], 'str_col'] = 'pink_floyd'
        
        # Add one record to df2
        new_record = {
            'id': n_records + 1,
            'int_col': 100500,
            'float_col': -0.42,
            'str_col': 'limp_bizkit',
            'str_col2': 'alice_cooper',
            'str_col3': 'rammstein',
            'str_col4': 'him',
            'str_col5': 'nine inch nails',
            'str_col6': 'prodigy',
            'bool_col': True,
        }
        df2 = pd.concat([df2, pd.DataFrame([new_record])], ignore_index=True)
        
        start_time = time.time()
        stats, details = compare_dataframes(df1, df2, ['id'])
        execution_time = time.time() - start_time
        
        assert execution_time < 5.0  # Should complete in <5 seconds
        assert stats.final_diff_score > 0.0
        assert stats.final_diff_score < 0.1
    
    def test_compound_primary_key(self):
        """Test comparison with compound primary key"""
        df1 = pd.DataFrame({
            'id1': [1, 1, 2],
            'id2': ['a', 'b', 'a'],
            'value': [10, 20, 30]
        })
        
        df2 = pd.DataFrame({
            'id1': [1, 2, 2],
            'id2': ['a', 'a', 'b'],
            'value': [10, 30, 40]
        })
        
        stats, details = compare_dataframes(df1, df2, ['id1', 'id2'])
        
        assert stats.common_pk_rows == 2  # (1,a) and (2,a)
        assert stats.only_source_rows == 1  # (1,b)
        assert stats.only_target_rows == 1  # (2,b)
        expected_score = 50.0 * 0.15 + 50.0 * 0.15
        assert stats.final_diff_score == pytest.approx(expected_score, rel=1e-5)
    
    def test_duplicate_primary_keys_in_source(self):
        """Test handling of duplicate primary keys within source dataframe"""
        df1 = pd.DataFrame({
            'pk': [1, 1, 2, 3],  # Duplicate PK=1
            'value': ['A', 'B', 'C', 'D']
        })
        
        df2 = pd.DataFrame({
            'pk': [1, 2, 3, 4],
            'value': ['A', 'C', 'D', 'E']
        })
        
        stats, details = compare_dataframes(df1, df2, ['pk'], 3)
        assert stats.final_diff_score == pytest.approx(7.5, rel=1e-5)
    
    def test_get_dataframe_size_gb(self):
        """Test dataframe size calculation"""
        df = pd.DataFrame({'col': range(1000)})
        size_gb = get_dataframe_size_gb(df)
        
        assert size_gb > 0.0
        assert size_gb < 0.1
    
    def test_validate_dataframe_size_raises_on_exceed(self):
        """Test validation raises exception when size exceeds limit"""
        df = pd.DataFrame({'col': range(10_000_000)})  # Large dataframe
        
        with pytest.raises(ValueError, match="DataFrame size.*exceeds limit"):
            validate_dataframe_size(df, max_size_gb=0.01)  # 10MB limit
    
    def test_clean_recently_changed_data(self):
        """Test cleaning recently changed data"""
        df1 = pd.DataFrame({
            'id': [1, 2, 3, 4],
            'value': ['A', 'B', 'C', 'D'],
            'xrecently_changed': ['y', 'n', 'y', 'n']
        })
        
        df2 = pd.DataFrame({
            'id': [1, 2, 3, 5],
            'value': ['A', 'B', 'X', 'E'],
            'xrecently_changed': ['n', 'y', 'n', 'n']
        })
        
        df1_clean, df2_clean = clean_recently_changed_data(df1, df2, ['id'])
        
        # IDs with 'y' in either dataframe should be removed
        assert 2 not in df1_clean['id'].values  # 'y' in df2
        assert 3 not in df1_clean['id'].values  # 'y' in df1
        assert 1 not in df1_clean['id'].values  # 'y' in df1
        assert 'xrecently_changed' not in df1_clean.columns

    def test_compare_dataframes_different_keys(self):
        """Test comparison with different primary keys"""
        df1 = pd.DataFrame({
            'id': [1, 2, 3],
            'name': ['Alice', 'Bob', 'Charlie']
        })
        
        df2 = pd.DataFrame({
            'id': [1, 2, 4],
            'name': ['Alice', 'Bob', 'David']
        })
        
        stats, details = compare_dataframes(df1, df2, ['id'], 3)
        
        assert stats.only_source_rows == 1
        assert stats.only_target_rows == 1
        assert stats.common_pk_rows == 2
        # Expected: 1 source-only row (50%) + 1 target-only row (50%) out of 2 common rows
        # Final score = 50% * 0.15 + 50% * 0.15 = 15.0%
        expected_score = 50.0 * 0.15 + 50.0 * 0.15
        assert stats.final_diff_score == pytest.approx(expected_score, rel=1e-5)

    def test_compare_dataframes_missing_columns(self):
        """Test comparison with missing key columns"""
        df1 = pd.DataFrame({'id': [1], 'name': ['Alice']})
        df2 = pd.DataFrame({'name': ['Alice']})  # Missing id column
        
        with pytest.raises(ValueError):
            compare_dataframes(df1, df2, ['id'], 3)    


    def test_compound_primary_key_all_different(self):
        """Test compound primary key with completely different keys"""
        df1 = pd.DataFrame({
            'key1': [1, 2, 3],
            'key2': ['X', 'Y', 'Z'],
            'value': [10, 20, 30]
        })

        df2 = pd.DataFrame({
            'key1': [4, 5, 6],
            'key2': ['A', 'B', 'C'],
            'value': [40, 50, 60]
        })

        stats, details = compare_dataframes(df1, df2, ['key1', 'key2'], 3)

        assert stats.only_source_rows == 3
        assert stats.only_target_rows == 3
        assert stats.common_pk_rows == 0
        # Expected: 100% mismatch
        assert stats.final_diff_score == pytest.approx(100.0, rel=1e-5)

    def test_compound_primary_key_partial_overlap(self):
        """Test compound primary key with partial overlap and value discrepancies"""
        df1 = pd.DataFrame({
            'id': [1, 1, 2, 3, 4],
            'type': ['A', 'B', 'A', 'A', 'B'],
            'amount': [100.0, 200.0, 300.0, 400.0, 500.0],
            'status': ['active', 'inactive', 'active', 'pending', 'active']
        })

        df2 = pd.DataFrame({
            'id': [1, 1, 2, 3, 5],
            'type': ['A', 'B', 'A', 'A', 'A'],
            'amount': [100.0, 250.0, 300.0, 450.0, 600.0],
            'status': ['active', 'inactive', 'active', 'completed', 'active']
        })

        stats, details = compare_dataframes(df1, df2, ['id', 'type'], 3)

        # Verify key statistics
        assert stats.common_pk_rows == 4  # (1,A), (1,B), (2,A), (3,A)
        assert stats.only_source_rows == 1  # (4,B)
        assert stats.only_target_rows == 1  # (5,A)

        # Should have discrepancies in values for some common keys
        # Expected: 1 source-only (25%) + 1 target-only (25%) + value mismatches
        assert stats.final_diff_score > 10.0
        assert stats.final_diff_score < 50.0

    def test_edge_case_all_different(self):
        """Test edge case where all records are different (from unittest)"""
        df1 = pd.DataFrame({
            'id': [1, 2, 3],
            'value': ['a', 'b', 'c']
        })

        df2 = pd.DataFrame({
            'id': [4, 5, 6],
            'value': ['x', 'y', 'z']
        })

        stats, details = compare_dataframes(df1, df2, ['id'], 3)

        assert stats.only_source_rows == 3
        assert stats.only_target_rows == 3
        assert stats.common_pk_rows == 0
        # Expected: 100% mismatch
        assert stats.final_diff_score == pytest.approx(100.0, rel=1e-5)

    def test_edge_case_complete_match(self):
        """Test edge case where everything matches perfectly (from unittest)"""
        df1 = pd.DataFrame({
            'id': [1, 2, 3],
            'value': ['a', 'b', 'c']
        })

        df2 = df1.copy()

        stats, details = compare_dataframes(df1, df2, ['id'], 3)

        assert stats.final_diff_score == pytest.approx(0.0, rel=1e-5)
        assert stats.total_matched_rows == 3

    def test_compound_primary_key_with_duplicates(self):
        """Test comparison with compound primary key and duplicate keys in source data (from unittest)"""
        df1 = pd.DataFrame({
            'id1': [1, 1, 2, 3],
            'id2': ['a', 'a', 'a', 'c'],
            'value': [10, 15, 30, 40]  # Duplicate (1,a) with different values
        })

        df2 = pd.DataFrame({
            'id1': [1, 2, 3],
            'id2': ['a', 'a', 'c'],
            'value': [11, 30, 40]
        })

        stats, details = compare_dataframes(df1, df2, ['id1', 'id2'])
        
        # With duplicates and value mismatches, score should be > 0
        # Duplicate in source contributes to source_dup_percentage
        assert stats.final_diff_score > 0.0
        # Should be significant due to duplicates
        assert stats.final_diff_score > 10.0

    def test_compound_primary_key_complex(self):
        """Test complex scenario with compound primary key and various discrepancies (from unittest)"""
        df1 = pd.DataFrame({
            'user_id':    [ 1,   1,   2,   3,   4],
            'session_id': ['A', 'B', 'A', 'A', 'A'],
            'value1': [100, 200, 300, 400, 500],
            'value2': ['X', 'Y', 'Z', 'W', 'V']
        })

        df2 = pd.DataFrame({
            'user_id':    [ 1,   2,   3,   4,   5],
            'session_id': ['A', 'A', 'A', 'B', 'A'],
            'value1': [100, 300, 400, 550, 600],
            'value2': ['X', 'Z', 'W', 'V', 'U']
        })

        stats, details = compare_dataframes(df1, df2, ['user_id', 'session_id'], 3)

        # Verify statistics
        assert stats.total_source_rows == 5
        assert stats.total_target_rows == 5
        assert stats.common_pk_rows == 3  # (1,A), (2,A), (3,A)
        assert stats.only_source_rows == 2  # (1,B), (4,A)
        assert stats.only_target_rows == 2  # (4,B), (5,A)
        assert stats.total_matched_rows == 3  # Only (1,A) has all values matching
        
        # Expected: 2 source-only (66.67%) + 2 target-only (66.67%) out of 3 common rows
        # No value mismatches in common rows
        assert stats.final_diff_score == pytest.approx(20, rel=1e-5) 

    def test_compound_primary_key_perfect_match(self):
        """Test compound primary key with perfect match (from unittest)"""
        df1 = pd.DataFrame({
            'part1': [1, 1, 2, 2],
            'part2': ['A', 'B', 'A', 'B'],
            'data': ['foo', 'bar', 'baz', 'qux']
        })

        df2 = pd.DataFrame({
            'part1': [1, 1, 2, 2],
            'part2': ['A', 'B', 'A', 'B'],
            'data': ['foo', 'bar', 'baz', 'qux']
        })

        stats, details = compare_dataframes(df1, df2, ['part1', 'part2'], 3)

        assert stats.final_diff_score == pytest.approx(0.0, rel=1e-5)
        assert stats.total_matched_rows == 4
        assert stats.common_pk_rows == 4         

    def test_duplicate_primary_keys_in_target(self):
        """Test handling of duplicate primary keys within target dataframe (from unittest)"""
        df1 = pd.DataFrame({
            'pk': [1, 2, 3, 4],
            'value': ['A', 'C', 'D', 'E']
        })

        df2 = pd.DataFrame({
            'pk': [1, 1, 2, 3],  # Duplicate PK=1
            'value': ['A', 'B', 'C', 'D']
        })

        stats, details = compare_dataframes(df1, df2, ['pk'], 3)
        
        assert stats.only_source_rows == 1
        assert stats.only_target_rows == 0
        assert stats.dup_source_rows == 0
        assert stats.dup_target_rows == 1
        assert stats.common_pk_rows == 3

        assert stats.dup_source_percentage_rows == 0
        assert stats.dup_target_percentage_rows == 25
        assert stats.source_only_percentage_rows == pytest.approx(33.33333, rel=1e-3) 
        assert stats.target_only_percentage_rows == 0

        expected_score = 25.0 * 0.1 + 33.33333 * 0.15
        assert stats.final_diff_score == pytest.approx(expected_score, rel=1e-5)   

    def test_duplicate_compound_primary_keys(self):
        """Test handling of duplicate compound primary keys (from unittest)"""
        df1 = pd.DataFrame({
            'key1':  [  1,   1,   1,   2],
            'key2':  ['A', 'A', 'B', 'A'],  # Duplicate (1,A)
            'value': [ 10,  20,  30,  40]
        })

        df2 = pd.DataFrame({
            'key1': [1, 1, 2, 3],
            'key2': ['A', 'B', 'A', 'A'],
            'value': [10, 30, 40, 50]
        })


        stats, details = compare_dataframes(df1, df2, ['key1', 'key2'], 3)
        assert stats.common_pk_rows == 3  
        assert stats.dup_source_percentage_rows == (1/4)*100
        assert stats.dup_target_percentage_rows == 0
        assert stats.total_matched_rows == 3
        assert stats.only_source_rows == 0
        assert stats.only_target_rows == 1

        # With duplicates and only trg rows mismatches
        expected_score = (1/4 * 0.1 + 1/3 * 0.15)*100
        assert stats.final_diff_score == pytest.approx(expected_score, rel=1e-5)  # Reduced precision             

@pytest.fixture
def sample_dataframe():
    """Fixture providing sample dataframe for tests"""
    return pd.DataFrame({
        'id': [1, 2, 3, 4],
        'name': ['Alice', 'Bob', 'Charlie', 'Diana'],
        'score': [85.5, 92.0, 78.5, 95.0]
    })





==================================================
ФАЙЛ: tests/integration/conftest.py
РАЗМЕР: 7388 символов
==================================================


import pytest
from sqlalchemy import create_engine, text, exc
import time
import logging
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type

logger = logging.getLogger(__name__)

@retry(
    stop=stop_after_attempt(12),
    wait=wait_fixed(10),
    retry=retry_if_exception_type((exc.OperationalError, exc.DBAPIError)),
    reraise=True
)
def wait_for_database(engine, db_name: str):
    """Wait for database to be ready with retry logic"""
    logger.info(f"Waiting for {db_name} to be ready...")
    with engine.begin() as conn:
        conn.execute(text("SELECT id FROM imalive"))

    logger.info(f"{db_name} is ready!")

@pytest.fixture(scope="session")
def postgres_engine():
    """PostgreSQL SQLAlchemy engine"""
    engine = create_engine(
        "postgresql+psycopg2://test_user:test_pass@localhost:5433/test_db",
        pool_pre_ping=True,
        pool_recycle=3600,
        connect_args={"connect_timeout": 10}
    )
    wait_for_database(engine, "PostgreSQL")
    return engine

@pytest.fixture(scope="session")
def oracle_engine():
    """Oracle SQLAlchemy engine"""
    engine = create_engine(
        "oracle+oracledb://test:test_pass@localhost:1521/?service_name=test_db",
        pool_pre_ping=True,
        pool_recycle=3600
    )
    wait_for_database(engine, "Oracle")
    return engine

@pytest.fixture(scope="session")
def clickhouse_engine():
    """ClickHouse SQLAlchemy engine"""
    engine = create_engine(
        "clickhouse+native://test_user:test_pass@localhost:9000/test",
        pool_recycle=3600
    )
    wait_for_database(engine, "ClickHouse")
    return engine

# Helper фикстура для создания/удаления тестовых таблиц
@pytest.fixture
def db_cleanup():
    """Yield None and cleanup after test if needed"""
    yield

# tests/integration/conftest.py (дополняем класс DBHelper)

class DBHelper:
    """Helper class for managing test tables"""
    
    def __init__(self):
        self._cleanup_stack = []
    
    @staticmethod
    def get_drop_sql(engine, table_name: str, object_type: str = "table") -> str:
        """Get drop SQL for specific database and object type"""
        dialect = engine.dialect.name
        
        if dialect == 'oracle':
            if object_type == "view":
                return f"""
                    BEGIN
                        EXECUTE IMMEDIATE 'DROP VIEW {table_name} CASCADE CONSTRAINTS';
                    EXCEPTION
                        WHEN OTHERS THEN
                            IF SQLCODE != -942 THEN
                                RAISE;
                            END IF;
                    END;
                """
            elif object_type == "materialized_view":
                return f"""
                    BEGIN
                        EXECUTE IMMEDIATE 'DROP MATERIALIZED VIEW {table_name}';
                    EXCEPTION
                        WHEN OTHERS THEN
                            IF SQLCODE != -942 THEN
                                RAISE;
                            END IF;
                    END;
                """
            else:
                return f"""
                    BEGIN
                        EXECUTE IMMEDIATE 'DROP TABLE {table_name} CASCADE CONSTRAINTS';
                    EXCEPTION
                        WHEN OTHERS THEN
                            IF SQLCODE != -942 THEN
                                RAISE;
                            END IF;
                    END;
                """
        elif dialect == 'clickhouse':
            if object_type == "view":
                return f"DROP VIEW IF EXISTS {table_name}"
            else:
                return f"DROP TABLE IF EXISTS {table_name}"
        elif dialect in ('postgresql', 'postgres'):
            if object_type == "view":
                return f"DROP VIEW IF EXISTS {table_name} CASCADE"
            elif object_type == "materialized_view":
                return f"DROP MATERIALIZED VIEW IF EXISTS {table_name} CASCADE"
            else:
                return f"DROP TABLE IF EXISTS {table_name} CASCADE"
        else:
            raise ValueError(f"Unsupported dialect: {dialect}")
    
    def create_table(self, engine, table_name: str, create_sql: str, 
                    insert_sql: str = None) -> None:
        """
        Create a test table and register it for automatic cleanup
        """
        drop_sql = self.get_drop_sql(engine, table_name, "table")
        
        # Clean up if exists
        with engine.begin() as conn:
            try:
                conn.execute(text(drop_sql))
            except Exception:
                pass  # Table might not exist
        
        # Create table
        with engine.begin() as conn:
            conn.execute(text(create_sql))
            if insert_sql:
                conn.execute(text(insert_sql))
        
        # Register for cleanup
        self._cleanup_stack.append((engine, table_name, drop_sql))
    
    def create_view(self, engine, view_name: str, view_sql: str, 
                   schema: str = "test") -> None:
        """
        Create a view and register it for automatic cleanup
        """
        full_view_name = f"{schema}.{view_name}" if schema else view_name
        drop_sql = self.get_drop_sql(engine, full_view_name, "view")
        
        # Clean up if exists
        with engine.begin() as conn:
            try:
                conn.execute(text(drop_sql))
            except Exception:
                pass  # View might not exist
        
        # Create view
        with engine.begin() as conn:
            conn.execute(text(view_sql))
        
        # Register for cleanup
        self._cleanup_stack.append((engine, full_view_name, drop_sql))
    
    def create_materialized_view(self, engine, mv_name: str, mv_sql: str,
                                schema: str = "test") -> None:
        """
        Create a materialized view and register it for automatic cleanup
        """
        full_mv_name = f"{schema}.{mv_name}" if schema else mv_name
        
        # Determine if database supports materialized views
        dialect = engine.dialect.name
        if dialect == 'clickhouse':
            # ClickHouse doesn't have materialized views in traditional sense
            return self.create_table(engine, mv_name, mv_sql, schema=schema)
        
        drop_sql = self.get_drop_sql(engine, full_mv_name, "materialized_view")
        
        # Clean up if exists
        with engine.begin() as conn:
            try:
                conn.execute(text(drop_sql))
            except Exception:
                pass
        
        # Create materialized view
        with engine.begin() as conn:
            conn.execute(text(mv_sql))
        
        # Register for cleanup
        self._cleanup_stack.append((engine, full_mv_name, drop_sql))
    
    def cleanup(self) -> None:
        """Cleanup all registered objects in reverse order"""
        for engine, object_name, drop_sql in reversed(self._cleanup_stack):
            with engine.begin() as conn:
                try:
                    conn.execute(text(drop_sql))
                except Exception:
                    pass
        self._cleanup_stack.clear()


@pytest.fixture
def table_helper():
    """Fixture providing DBHelper instance"""
    helper = DBHelper()
    yield helper
    #helper.cleanup() #do not cleanup for the debug


==================================================
ФАЙЛ: tests/integration/cross_db/clickhouse_oracle/test_ch_ora_sample_discrepant.py
РАЗМЕР: 4340 символов
==================================================

"""
Test sample comparison with intentional discrepancies between ClickHouse and Oracle.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestClickHouseOracleDataWithDiscrepancies:
    """Cross-database sample comparison with discrepancies"""
    
    @pytest.fixture(autouse=True)
    def setup_data_with_discrepancies(self, clickhouse_engine, oracle_engine):
        """Setup test data with intentional discrepancies"""
        
        table_name = "test_ch_ora_discrepancies"
        
        # ClickHouse setup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id UInt32,
                    name String,
                    amount Decimal(15,2),
                    transaction_date Date,
                    updated_at DateTime,
                    is_active UInt8
                )
                ENGINE = MergeTree()
                ORDER BY id
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, name, amount, transaction_date, updated_at, is_active) VALUES
                (1, 'Transaction A', 1000.50, '2024-01-01', '2024-01-01 10:00:00', 1),
                (2, 'Transaction B', 2500.75, '2024-01-02', '2024-01-02 11:30:00', 1),
                (3, 'Transaction C', 500.00, '2024-01-03', '2024-01-03 14:45:00', 0)
            """))
        
        # Oracle setup - with discrepancies
        with oracle_engine.begin() as conn:
            conn.execute(text(f"""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE {table_name} CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id NUMBER PRIMARY KEY,
                    name VARCHAR2(100),
                    amount NUMBER(15,2),
                    transaction_date DATE,
                    updated_at TIMESTAMP,
                    is_active NUMBER(1) CHECK (is_active IN (0, 1))
                )
            """))
            
            # Different amount for id=1, missing record id=2
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, name, amount, transaction_date, updated_at, is_active) VALUES
                (1, 'Transaction A', 1001.50, DATE '2024-01-01', TIMESTAMP '2024-01-01 10:00:00', 1),
                (3, 'Transaction C', 500.00, DATE '2024-01-03', TIMESTAMP '2024-01-03 14:45:00', 0)
            """))
        
        yield
        
        # Cleanup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
        
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text(f"DROP TABLE {table_name} CASCADE CONSTRAINTS"))
            except:
                pass

    def test_sample_comparison_with_discrepancies(self, clickhouse_engine, oracle_engine):
        """
        Test sample comparison with intentional discrepancies.
        """
        table_name = "test_ch_ora_discrepancies"
        
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="transaction_date",
            update_column="updated_at",
            date_range=("2024-01-01", "2024-01-05"),
            exclude_recent_hours=24,
            tolerance_percentage=35.0,  # Allow 5% tolerance
        )
        print (report)
        assert status == COMPARISON_SUCCESS  # Should pass with tolerance
        assert stats.final_diff_score > 0.0
        print(f"ClickHouse → Oracle with discrepancies comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/clickhouse_oracle/test_ch_ora_sample_identical.py
РАЗМЕР: 3841 символов
==================================================

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestClickHouseOracleIdenticalData:
    """Cross-database identical data sample comparison tests"""
    
    @pytest.fixture(autouse=True)
    def setup_identical_data(self, clickhouse_engine, oracle_engine, table_helper):
        """Setup identical test data for ClickHouse vs Oracle"""
        
        table_name = "test_ch_ora_identical"
        
        table_helper.create_table(
            engine=clickhouse_engine,
            table_name=table_name,
            create_sql=f"""
                CREATE TABLE {table_name} (
                    id UInt32,
                    name String,
                    amount Decimal(15,2),
                    transaction_date Date,
                    updated_at DateTime,
                    is_active UInt8,
                    category Nullable(String)
                )
                ENGINE = MergeTree()
                ORDER BY id
            """,
            insert_sql=f"""
                        INSERT INTO {table_name} VALUES
                        (1, 'Transaction A', 1000.50, '2024-01-01', '2024-01-01 10:00:00', 1, 'Category 1'),
                        (2, 'Transaction B', 2500.75, '2024-01-02', '2024-01-02 11:30:00', 1, 'Category 2'),
                        (3, 'Transaction C', 500.00, '2024-01-03', '2024-01-03 14:45:00', 0, NULL),
                        (4, 'Transaction D', 750.25, '2024-01-04', '2024-01-04 09:15:00', 1, 'Category 3')
                    """
        )
                
        table_helper.create_table(
            engine=oracle_engine,
            table_name=table_name,
            create_sql=f"""
                        CREATE TABLE {table_name} (
                            id NUMBER PRIMARY KEY,
                            name VARCHAR2(100),
                            amount NUMBER(15,2),
                            transaction_date DATE,
                            updated_at TIMESTAMP,
                            is_active NUMBER(1) CHECK (is_active IN (0, 1)),
                            category VARCHAR2(100)
                        )
                    """,
            insert_sql=f"""
                INSERT INTO {table_name} VALUES
                (1, 'Transaction A', 1000.50, DATE '2024-01-01', TIMESTAMP '2024-01-01 10:00:00', 1, 'Category 1'),
                (2, 'Transaction B', 2500.75, DATE '2024-01-02', TIMESTAMP '2024-01-02 11:30:00', 1, 'Category 2'),
                (3, 'Transaction C', 500.00, DATE '2024-01-03', TIMESTAMP '2024-01-03 14:45:00', 0, NULL),
                (4, 'Transaction D', 750.25, DATE '2024-01-04', TIMESTAMP '2024-01-04 09:15:00', 1, 'Category 3')
            """
        )
        
        yield
        

    def test_identical_data_sample_comparison(self, clickhouse_engine, oracle_engine):
        """
        Test sample comparison between identical data in ClickHouse and Oracle.
        """
        table_name = "test_ch_ora_identical"
        
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="transaction_date",
            update_column="updated_at",
            date_range=("2024-01-01", "2024-01-05"),
            exclude_recent_hours=24,
            tolerance_percentage=0.0,
        )
        
        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"ClickHouse → Oracle identical data comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/clickhouse_oracle/test_ch_ora_count_identical.py
РАЗМЕР: 3727 символов
==================================================

"""
Test count-based comparison between ClickHouse and Oracle.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestClickHouseOracleCountsComparison:
    """Cross-database count-based comparison tests"""
    
    @pytest.fixture(autouse=True)
    def setup_count_data(self, clickhouse_engine, oracle_engine):
        """Setup test data for count comparison"""
        
        table_name = "test_ch_ora_counts"
        
        # ClickHouse setup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id UInt32,
                    event_date Date,
                    event_type String
                )
                ENGINE = MergeTree()
                ORDER BY id
            """))
            
            # 3 records on 2024-01-01, 2 on 2024-01-02
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, event_date, event_type) VALUES
                (1, '2024-01-01', 'login'),
                (2, '2024-01-01', 'purchase'),
                (3, '2024-01-01', 'logout'),
                (4, '2024-01-02', 'login'),
                (5, '2024-01-02', 'view')
            """))
        
        # Oracle setup
        with oracle_engine.begin() as conn:
            conn.execute(text(f"""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE {table_name} CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id NUMBER PRIMARY KEY,
                    event_date DATE,
                    event_type VARCHAR2(50)
                )
            """))
            
            # Same counts
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, event_date, event_type) VALUES
                (1, DATE '2024-01-01', 'login'),
                (2, DATE '2024-01-01', 'purchase'),
                (3, DATE '2024-01-01', 'logout'),
                (4, DATE '2024-01-02', 'login'),
                (5, DATE '2024-01-02', 'view')
            """))
        
        yield
        
        # Cleanup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
        
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text(f"DROP TABLE {table_name} CASCADE CONSTRAINTS"))
            except:
                pass

    def test_counts_comparison(self, clickhouse_engine, oracle_engine):
        """
        Test count-based comparison between ClickHouse and Oracle.
        """
        table_name = "test_ch_ora_counts"
        
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_counts(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="event_date",
            date_range=("2024-01-01", "2024-01-03"),
            tolerance_percentage=0.0,
        )
        print(report)
        assert status == COMPARISON_SUCCESS
        print(f"ClickHouse → Oracle count comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/clickhouse_postgres/test_ch_pg_sample_identical_numeric.py
РАЗМЕР: 3010 символов
==================================================

"""
Test ClickHouse numeric types comparison with PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS


class TestClickHouseNumericTypes:
    """Tests for ClickHouse numeric types comparison with PostgreSQL"""
    
    @pytest.fixture(autouse=True)
    def setup_clickhouse_numeric_data(self, clickhouse_engine, postgres_engine):
        """Setup numeric test data for ClickHouse vs PostgreSQL"""
        # ClickHouse
        with clickhouse_engine.begin() as conn:
            conn.execute(text("DROP TABLE IF EXISTS test_ch_numerics"))
            
            conn.execute(text("""
                CREATE TABLE test_ch_numerics (
                    id UInt32,
                    price Decimal(10,2),
                    quantity UInt32,
                    discount Float64,
                    created_at Date
                )
                ENGINE = MergeTree()
                ORDER BY id
            """))
            
            conn.execute(text("""
                INSERT INTO test_ch_numerics (id, price, quantity, discount, created_at) VALUES
                (1, 100.50, 10, 0.1, '2024-01-01'),
                (2, 250.75, 5, 0.15, '2024-01-02'),
                (3, 99.99, 20, 0.05, '2024-01-03')
            """))
        
        # PostgreSQL
        with postgres_engine.begin() as conn:
            conn.execute(text("DROP TABLE IF EXISTS test_ch_numerics CASCADE"))
            
            conn.execute(text("""
                CREATE TABLE test_ch_numerics (
                    id INTEGER PRIMARY KEY,
                    price NUMERIC(10,2),
                    quantity INTEGER,
                    discount DOUBLE PRECISION,
                    created_at DATE
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_ch_numerics (id, price, quantity, discount, created_at) VALUES
                (1, 100.50, 10, 0.1, '2024-01-01'),
                (2, 250.75, 5, 0.15, '2024-01-02'),
                (3, 99.99, 20, 0.05, '2024-01-03')
            """))
        
        yield

    def test_clickhouse_numeric_types_comparison(self, clickhouse_engine, postgres_engine):
        """
        Compare numeric types between ClickHouse and PostgreSQL.
        """
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_ch_numerics", "test"),
            target_table=DataReference("test_ch_numerics", "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-05"),
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        print(f"ClickHouse numeric types comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/clickhouse_postgres/test_ch_pg_sample_identical_compound_pk.py
РАЗМЕР: 3924 символов
==================================================

"""
Test sample comparison with compound primary key between ClickHouse and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestClickHousePostgresCompoundKey:
    """Cross-database sample comparison with compound primary key"""
    
    @pytest.fixture(autouse=True)
    def setup_compound_key_data(self, clickhouse_engine, postgres_engine):
        """Setup test data with compound primary key"""
        
        table_name = "test_ch_pg_compound_key"
        
        # ClickHouse setup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    user_id UInt32,
                    session_id String,
                    page_views UInt32,
                    duration Float64,
                    event_date Date,
                    device_type String
                )
                ENGINE = MergeTree()
                ORDER BY (user_id, session_id)
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (user_id, session_id, page_views, duration, event_date, device_type) VALUES
                (1001, 'sess_001', 15, 120.5, '2024-01-01', 'mobile'),
                (1002, 'sess_002', 8, 45.25, '2024-01-01', 'desktop'),
                (1001, 'sess_004', 12, 95.75, '2024-01-02', 'mobile')
            """))
        
        # PostgreSQL setup
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    user_id INTEGER,
                    session_id TEXT,
                    page_views INTEGER,
                    duration DOUBLE PRECISION,
                    event_date DATE,
                    device_type TEXT
                )
            """))
            
            conn.execute(text(f"""
                ALTER TABLE {table_name} 
                ADD PRIMARY KEY (user_id, session_id)
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (user_id, session_id, page_views, duration, event_date, device_type) VALUES
                (1001, 'sess_001', 15, 120.5, '2024-01-01', 'mobile'),
                (1002, 'sess_002', 8, 45.25, '2024-01-01', 'desktop'),
                (1001, 'sess_004', 12, 95.75, '2024-01-02', 'mobile')
            """))
        
        yield
        
        # Cleanup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
        
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))

    def test_sample_with_compound_key(self, clickhouse_engine, postgres_engine):
        """
        Test sample comparison with compound primary key.
        """
        table_name = "test_ch_pg_compound_key"
        
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="event_date",
            update_column=None,
            date_range=("2024-01-01", "2024-01-03"),
            exclude_recent_hours=24,
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"ClickHouse → PostgreSQL compound key comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/clickhouse_postgres/test_ch_pg_sample_identical_null.py
РАЗМЕР: 3103 символов
==================================================

"""
Test NULL values comparison between ClickHouse and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS


class TestClickHouseNullValues:
    """Tests for NULL values comparison with ClickHouse"""
    
    @pytest.fixture(autouse=True)
    def setup_clickhouse_null_data(self, clickhouse_engine, postgres_engine):
        """Setup NULL test data for ClickHouse vs PostgreSQL"""
        # ClickHouse
        with clickhouse_engine.begin() as conn:
            conn.execute(text("DROP TABLE IF EXISTS test_ch_nulls"))
            
            conn.execute(text("""
                CREATE TABLE test_ch_nulls (
                    id UInt32,
                    nullable_string Nullable(String),
                    nullable_number Nullable(Int32),
                    nullable_date Nullable(Date),
                    created_at Date
                )
                ENGINE = MergeTree()
                ORDER BY id
            """))
            
            conn.execute(text("""
                INSERT INTO test_ch_nulls (id, nullable_string, nullable_number, nullable_date, created_at) VALUES
                (1, NULL, NULL, NULL, '2024-01-01'),
                (2, 'Some text', 123, '2024-01-02', '2024-01-02'),
                (3, NULL, 456, NULL, '2024-01-03')
            """))
        
        # PostgreSQL
        with postgres_engine.begin() as conn:
            conn.execute(text("DROP TABLE IF EXISTS test_ch_nulls CASCADE"))
            
            conn.execute(text("""
                CREATE TABLE test_ch_nulls (
                    id INTEGER PRIMARY KEY,
                    nullable_string TEXT,
                    nullable_number INTEGER,
                    nullable_date DATE,
                    created_at DATE NOT NULL
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_ch_nulls (id, nullable_string, nullable_number, nullable_date, created_at) VALUES
                (1, NULL, NULL, NULL, '2024-01-01'),
                (2, 'Some text', 123, '2024-01-02', '2024-01-02'),
                (3, NULL, 456, NULL, '2024-01-03')
            """))
        
        yield

    def test_clickhouse_null_values_comparison(self, clickhouse_engine, postgres_engine):
        """
        Compare tables with NULL values between ClickHouse and PostgreSQL.
        """
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_ch_nulls", "test"),
            target_table=DataReference("test_ch_nulls", "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-05"),
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        print(f"ClickHouse NULL values comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/clickhouse_postgres/test_ch_pg_count_identical.py
РАЗМЕР: 3183 символов
==================================================

"""
Test count-based comparison between ClickHouse and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestClickHousePostgresCountsComparison:
    """Cross-database count-based comparison tests ClickHouse ↔ PostgreSQL"""
    
    @pytest.fixture(autouse=True)
    def setup_count_data(self, clickhouse_engine, postgres_engine):
        """Setup test data for count comparison"""
        
        table_name = "test_ch_pg_counts"
        
        # ClickHouse setup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id UInt32,
                    event_date Date,
                    event_type String
                )
                ENGINE = MergeTree()
                ORDER BY id
            """))
            
            # 3 records on 2024-01-01, 2 on 2024-01-02
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, event_date, event_type) VALUES
                (1, '2024-01-01', 'login'),
                (2, '2024-01-01', 'purchase'),
                (3, '2024-01-01', 'logout'),
                (4, '2024-01-02', 'login'),
                (5, '2024-01-02', 'view')
            """))
        
        # PostgreSQL setup
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id INTEGER PRIMARY KEY,
                    event_date DATE,
                    event_type TEXT
                )
            """))
            
            # Same counts
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, event_date, event_type) VALUES
                (1, '2024-01-01', 'login'),
                (2, '2024-01-01', 'purchase'),
                (3, '2024-01-01', 'logout'),
                (4, '2024-01-02', 'login'),
                (5, '2024-01-02', 'view')
            """))
        
        yield
        
        

    def test_counts_comparison(self, clickhouse_engine, postgres_engine):
        """
        Test count-based comparison between ClickHouse and PostgreSQL.
        """
        table_name = "test_ch_pg_counts"
        
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_counts(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="event_date",
            date_range=("2024-01-01", "2024-01-03"),
            tolerance_percentage=0.0,
        )
        print(report)
        assert status == COMPARISON_SUCCESS
        assert stats.final_score == 100.0
        print(f"ClickHouse → PostgreSQL count comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/clickhouse_postgres/test_ch_pg_sample_identical_excluded_cols.py
РАЗМЕР: 3480 символов
==================================================

"""
Test sample comparison with column exclusion between ClickHouse and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestClickHousePostgresColumnExclusion:
    """Cross-database sample comparison with column exclusion"""
    
    @pytest.fixture(autouse=True)
    def setup_column_exclusion_data(self, clickhouse_engine, postgres_engine):
        """Setup test data for column exclusion test"""
        
        table_name = "test_ch_pg_col_exclusion"
        
        # ClickHouse setup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id UInt32,
                    name String,
                    created_at DateTime,
                    internal_id UInt32,
                    public_data String
                )
                ENGINE = MergeTree()
                ORDER BY id
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, name, created_at, internal_id, public_data) VALUES
                (1, 'Item A', '2024-01-01 10:00:00', 999, 'Public A'),
                (2, 'Item B', '2024-01-02 11:00:00', 888, 'Public B')
            """))
        
        # PostgreSQL setup
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id INTEGER PRIMARY KEY,
                    name TEXT,
                    created_at TIMESTAMP,
                    internal_id INTEGER,
                    public_data TEXT
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, name, created_at, internal_id, public_data) VALUES
                (1, 'Item A', '2024-01-01 10:00:00', 999, 'Public A'),
                (2, 'Item B', '2024-01-02 11:00:00', 888, 'Public B')
            """))
        
        yield
        
        # Cleanup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
        
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))

    def test_sample_with_column_exclusion(self, clickhouse_engine, postgres_engine):
        """
        Test sample comparison with excluded columns.
        """
        table_name = "test_ch_pg_col_exclusion"
        
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-03"),
            exclude_columns=["internal_id"],  # Exclude internal column
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"ClickHouse → PostgreSQL with column exclusion passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_sample_identical_numeric.py
РАЗМЕР: 3575 символов
==================================================

"""
Test numeric type comparison between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestNumericTypesComparison:
    """Tests for numeric type comparison"""
    
    @pytest.fixture(autouse=True)
    def setup_numeric_data(self, oracle_engine, postgres_engine):
        """Setup numeric test data"""
        
        table_name = "test_types_numeric"
        
        # Oracle
        with oracle_engine.begin() as conn:
            conn.execute(text(f"""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE {table_name} CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id NUMBER PRIMARY KEY,
                    price NUMBER(10,2),
                    quantity INTEGER,
                    discount FLOAT,
                    created_at DATE
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, price, quantity, discount, created_at) VALUES
                (1, 100.50, 10, 0.1, DATE '2024-01-01'),
                (2, 250.75, 5, 0.15, DATE '2024-01-02'),
                (3, 99.99, 20, 0.05, DATE '2024-01-03')
            """))
        
        # PostgreSQL
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id INTEGER PRIMARY KEY,
                    price NUMERIC(10,2),
                    quantity INTEGER,
                    discount DOUBLE PRECISION,
                    created_at DATE
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, price, quantity, discount, created_at) VALUES
                (1, 100.50, 10, 0.1, '2024-01-01'),
                (2, 250.75, 5, 0.15, '2024-01-02'),
                (3, 99.99, 20, 0.05, '2024-01-03')
            """))
        
        yield
        
        # Cleanup
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text(f"DROP TABLE {table_name} CASCADE CONSTRAINTS"))
            except:
                pass
        
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))

    def test_numeric_types_comparison(self, oracle_engine, postgres_engine):
        """
        Compare numeric types: Oracle NUMBER vs PostgreSQL NUMERIC.
        """
        table_name = "test_types_numeric"
        
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-05"),
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        print(f"Numeric types comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_count_identical.py
РАЗМЕР: 3739 символов
==================================================

"""
Test count-based comparison between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestOraclePostgresCountsComparison:
    """Cross-database count-based comparison tests Oracle ↔ PostgreSQL"""
    
    @pytest.fixture(autouse=True)
    def setup_count_data(self, oracle_engine, postgres_engine):
        """Setup test data for count comparison"""
        
        table_name = "test_ora_pg_counts"
        
        # Oracle setup
        with oracle_engine.begin() as conn:
            conn.execute(text(f"""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE {table_name} CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id NUMBER PRIMARY KEY,
                    event_date DATE,
                    event_type VARCHAR2(50)
                )
            """))
            
            # 3 records on 2024-01-01, 2 on 2024-01-02
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, event_date, event_type) VALUES
                (1, DATE '2024-01-01', 'login'),
                (2, DATE '2024-01-01', 'purchase'),
                (3, DATE '2024-01-01', 'logout'),
                (4, DATE '2024-01-02', 'login'),
                (5, DATE '2024-01-02', 'view')
            """))
        
        # PostgreSQL setup
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id INTEGER PRIMARY KEY,
                    event_date DATE,
                    event_type TEXT
                )
            """))
            
            # Same counts
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, event_date, event_type) VALUES
                (1, '2024-01-01', 'login'),
                (2, '2024-01-01', 'purchase'),
                (3, '2024-01-01', 'logout'),
                (4, '2024-01-02', 'login'),
                (5, '2024-01-02', 'view')
            """))
        
        yield
        
        # Cleanup
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text(f"DROP TABLE {table_name} CASCADE CONSTRAINTS"))
            except:
                pass
        
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))

    def test_counts_comparison(self, oracle_engine, postgres_engine):
        """
        Test count-based comparison between Oracle and PostgreSQL.
        """
        table_name = "test_ora_pg_counts"
        
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_counts(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="event_date",
            date_range=("2024-01-01", "2024-01-03"),
            tolerance_percentage=0.0,
        )
        print(report)
        assert status == COMPARISON_SUCCESS
        assert stats.final_score == 100.0
        print(f"Oracle → PostgreSQL count comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_sample_identical_unicode_chars.py
РАЗМЕР: 3163 символов
==================================================

"""
Test Unicode and special characters comparison between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS


class TestUnicodeComparison:
    """Tests for Unicode and special characters"""
    
    @pytest.fixture(autouse=True)
    def setup_unicode_data(self, oracle_engine, postgres_engine):
        """Setup Unicode test data"""
        # Oracle
        with oracle_engine.begin() as conn:
            conn.execute(text("""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE test_unicode CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text("""
                CREATE TABLE test_unicode (
                    id NUMBER PRIMARY KEY,
                    text_english VARCHAR2(200),
                    text_russian VARCHAR2(200),
                    text_emoji VARCHAR2(200),
                    created_date DATE
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_unicode (id, text_english, text_russian, text_emoji, created_date) VALUES
                (1, 'Hello World', 'Привет мир', '😀 🚀 📊', DATE '2024-01-01'),
                (2, 'Test data', 'Тестовые данные', '✅ ❌ ⚠️', DATE '2024-01-02')
            """))
        
        # PostgreSQL
        with postgres_engine.begin() as conn:
            conn.execute(text("DROP TABLE IF EXISTS test_unicode CASCADE"))
            
            conn.execute(text("""
                CREATE TABLE test_unicode (
                    id INTEGER PRIMARY KEY,
                    text_english TEXT,
                    text_russian TEXT,
                    text_emoji TEXT,
                    created_date DATE
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_unicode (id, text_english, text_russian, text_emoji, created_date) VALUES
                (1, 'Hello World', 'Привет мир', '😀 🚀 📊', '2024-01-01'),
                (2, 'Test data', 'Тестовые данные', '✅ ❌ ⚠️', '2024-01-02')
            """))
        
        yield

    def test_unicode_special_chars(self, oracle_engine, postgres_engine):
        """
        Compare strings with Unicode and special characters.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_unicode", "test"),
            target_table=DataReference("test_unicode", "test"),
            date_column="created_date",
            date_range=("2024-01-01", "2024-01-05"),
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        print(f"Unicode comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_custom_query_identical.py
РАЗМЕР: 3886 символов
==================================================

"""
Test custom query comparison between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS


class TestCustomQueryComparison:
    """Tests for custom query comparison"""
    
    @pytest.fixture(autouse=True)
    def setup_custom_data(self, oracle_engine, postgres_engine):
        """Setup custom query test data"""
        # Oracle
        with oracle_engine.begin() as conn:
            conn.execute(text("""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE test_custom_data CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text("""
                CREATE TABLE test_custom_data (
                    id          INTEGER PRIMARY KEY,
                    name        varchar2(256) NOT NULL,
                    created_at  DATE NOT NULL,
                    updated_at  TIMESTAMP NOT NULL
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_custom_data (id, name, created_at, updated_at) VALUES
                (1, 'Alice',   date'2024-01-01', timestamp'2024-01-01 10:00:00'),
                (2, 'Robert',  date'2024-01-02', timestamp'2024-01-02 11:00:00'),
                (3, 'Charlie', date'2024-01-03', timestamp'2024-01-03 12:00:00')
            """))
        
        # PostgreSQL
        with postgres_engine.begin() as conn:
            conn.execute(text("DROP TABLE IF EXISTS test_custom_data CASCADE"))
            
            conn.execute(text("""
                CREATE TABLE test_custom_data (
                    id          INTEGER PRIMARY KEY,
                    name        TEXT NOT NULL,
                    created_at  DATE NOT NULL,
                    updated_at  TIMESTAMP NOT NULL
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_custom_data (id, name, created_at, updated_at) VALUES
                (1, 'Alice',   '2024-01-01', '2024-01-01 10:00:00'),
                (2, 'Robert',  '2024-01-02', '2024-01-02 11:00:00'),
                (3, 'Charlie', '2024-01-03', '2024-01-03 12:00:00')
            """))
        
        yield

    def test_custom_query_comparison(self, oracle_engine, postgres_engine):
        """
        Test custom query comparison between databases.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        source_query = """
            SELECT id, name, created_at
            FROM test.test_custom_data
            WHERE created_at >= trunc(to_date(:start_date, 'YYYY-MM-DD'), 'dd')
              AND created_at < trunc(to_date(:end_date, 'YYYY-MM-DD'), 'dd') + 1
        """
        
        target_query = """
            SELECT id, name, created_at
            FROM test.test_custom_data
            WHERE created_at >= date_trunc('day', %(start_date)s::date)
              AND created_at < date_trunc('day', %(end_date)s::date) + interval '1 days'
        """

        status, report, stats, details = comparator.compare_custom_query(
            source_query=source_query,
            source_params={'start_date': '2024-01-01', 'end_date': '2024-01-03'},
            target_query=target_query,
            target_params={'start_date': '2024-01-01', 'end_date': '2024-01-03'},
            custom_primary_key=["id"],
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        print(f"Custom query comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_sample_identical.py
РАЗМЕР: 4058 символов
==================================================

"""
Test HR data comparison between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestOraclePostgresHRData:
    """HR data comparison between Oracle and PostgreSQL"""
    
    @pytest.fixture(autouse=True)
    def setup_hr_data(self, oracle_engine, postgres_engine):
        """Setup HR test data"""
        
        table_name = "test_ora_pg_hr"
        
        # Oracle setup
        with oracle_engine.begin() as conn:
            conn.execute(text(f"""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE {table_name} CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    employee_id NUMBER PRIMARY KEY,
                    first_name VARCHAR2(50),
                    last_name VARCHAR2(50),
                    email VARCHAR2(100),
                    hire_date DATE,
                    salary NUMBER(10,2),
                    department_id NUMBER
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} 
                (employee_id, first_name, last_name, email, hire_date, salary, department_id) 
                VALUES
                (101, 'John', 'Doe', 'john.doe@company.com', DATE '2020-01-15', 60000.00, 10),
                (102, 'Jane', 'Smith', 'jane.smith@company.com', DATE '2019-03-20', 75000.00, 20)
            """))
        
        # PostgreSQL setup
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    employee_id INTEGER PRIMARY KEY,
                    first_name TEXT,
                    last_name TEXT,
                    email TEXT,
                    hire_date DATE,
                    salary NUMERIC(10,2),
                    department_id INTEGER
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} 
                (employee_id, first_name, last_name, email, hire_date, salary, department_id) 
                VALUES
                (101, 'John', 'Doe', 'john.doe@company.com', '2020-01-15', 60000.00, 10),
                (102, 'Jane', 'Smith', 'jane.smith@company.com', '2019-03-20', 75000.00, 20)
            """))
        
        yield
        
        # Cleanup
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text(f"DROP TABLE {table_name} CASCADE CONSTRAINTS"))
            except:
                pass
        
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))

    def test_hr_data_comparison(self, oracle_engine, postgres_engine):
        """
        Test HR data comparison between Oracle and PostgreSQL.
        """
        table_name = "test_ora_pg_hr"
        
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="hire_date",
            update_column=None,
            date_range=("2018-01-01", "2022-01-01"),
            exclude_recent_hours=1,
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"Oracle → PostgreSQL HR data comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_sample_identical_custom_pk.py
РАЗМЕР: 3794 символов
==================================================

"""
Test comparison with custom primary key specification between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestOraclePostgresCustomPrimaryKey:
    """Comparison with custom primary key specification"""
    
    @pytest.fixture(autouse=True)
    def setup_custom_pk_data(self, oracle_engine, postgres_engine):
        """Setup test data for custom PK test"""
        
        table_name = "test_ora_pg_custom_pk"
        
        # Oracle setup
        with oracle_engine.begin() as conn:
            conn.execute(text(f"""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE {table_name} CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    user_id NUMBER,
                    email VARCHAR2(100),
                    name VARCHAR2(100),
                    created_date DATE
                )
            """))
            
            # Add duplicate email to test custom PK
            conn.execute(text(f"""
                INSERT INTO {table_name} (user_id, email, name, created_date) VALUES
                (1, 'user1@company.com', 'John Doe', DATE '2024-01-01'),
                (2, 'user1@company.com', 'Jane Smith', DATE '2024-01-02'),
                (3, 'user2@company.com', 'Bob Johnson', DATE '2024-01-03')
            """))
        
        # PostgreSQL setup
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    user_id INTEGER,
                    email TEXT,
                    name TEXT,
                    created_date DATE
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (user_id, email, name, created_date) VALUES
                (1, 'user1@company.com', 'John Doe', '2024-01-01'),
                (2, 'user1@company.com', 'Jane Smith', '2024-01-02'),
                (3, 'user2@company.com', 'Bob Johnson', '2024-01-03')
            """))
        
        yield
        
        # Cleanup
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text(f"DROP TABLE {table_name} CASCADE CONSTRAINTS"))
            except:
                pass
        
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))

    def test_with_custom_primary_key(self, oracle_engine, postgres_engine):
        """
        Test comparison with custom primary key specification.
        """
        table_name = "test_ora_pg_custom_pk"
        
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="created_date",
            date_range=("2024-01-01", "2024-01-05"),
            custom_primary_key=["email"],  # Custom PK by email
            tolerance_percentage=5.0,
        )

        # Should detect duplicates
        assert stats.dup_source_rows > 0
        print(f"Oracle → PostgreSQL with custom PK passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_sample_identical_dates.py
РАЗМЕР: 2924 символов
==================================================

"""
Test DATE type comparison between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS


class TestDateTypeComparison:
    """Tests for DATE type comparison"""
    
    @pytest.fixture(autouse=True)
    def setup_date_data(self, oracle_engine, postgres_engine):
        """Setup DATE test data"""
        # Oracle
        with oracle_engine.begin() as conn:
            conn.execute(text("""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE test_dates CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text("""
                CREATE TABLE test_dates (
                    id NUMBER PRIMARY KEY,
                    event_date DATE NOT NULL,
                    event_name VARCHAR2(100)
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_dates (id, event_date, event_name) VALUES
                (1, DATE '2024-01-01', 'New Year'),
                (2, DATE '2024-01-02', 'Second day'),
                (3, DATE '2024-01-03', 'Third day')
            """))
        
        # PostgreSQL
        with postgres_engine.begin() as conn:
            conn.execute(text("DROP TABLE IF EXISTS test_dates CASCADE"))
            
            conn.execute(text("""
                CREATE TABLE test_dates (
                    id INTEGER PRIMARY KEY,
                    event_date DATE NOT NULL,
                    event_name TEXT
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_dates (id, event_date, event_name) VALUES
                (1, '2024-01-01', 'New Year'),
                (2, '2024-01-02', 'Second day'),
                (3, '2024-01-03', 'Third day')
            """))
        
        yield

    def test_date_type_comparison(self, oracle_engine, postgres_engine):
        """
        Compare DATE type between Oracle and PostgreSQL.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_dates", "test"),
            target_table=DataReference("test_dates", "test"),
            date_column="event_date",
            date_range=("2024-01-01", "2024-01-10"),
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"Date type comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_sample_identical_boolean.py
РАЗМЕР: 3487 символов
==================================================

"""
Test boolean type comparison between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestBooleanComparison:
    """Tests for boolean type comparison"""
    
    @pytest.fixture(autouse=True)
    def setup_boolean_data(self, oracle_engine, postgres_engine):
        """Setup boolean test data"""
        
        table_name = "test_types_boolean"
        
        # Oracle: boolean as NUMBER(1)
        with oracle_engine.begin() as conn:
            conn.execute(text(f"""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE {table_name} CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id NUMBER PRIMARY KEY,
                    is_active NUMBER(1) CHECK (is_active IN (0, 1)),
                    created_at DATE
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, is_active, created_at) VALUES
                (1, 1, DATE '2024-01-01'),
                (2, 0, DATE '2024-01-02'),
                (3, 1, DATE '2024-01-03')
            """))
        
        # PostgreSQL: boolean as BOOLEAN
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id INTEGER PRIMARY KEY,
                    is_active BOOLEAN,
                    created_at DATE
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, is_active, created_at) VALUES
                (1, TRUE, '2024-01-01'),
                (2, FALSE, '2024-01-02'),
                (3, TRUE, '2024-01-03')
            """))
        
        yield
        
        # Cleanup
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text(f"DROP TABLE {table_name} CASCADE CONSTRAINTS"))
            except:
                pass
        
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))

    def test_boolean_comparison(self, oracle_engine, postgres_engine):
        """
        Compare boolean values between Oracle (0/1) and PostgreSQL (TRUE/FALSE).
        Adapters should handle type conversion.
        """
        table_name = "test_types_boolean"
        
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-05"),
            tolerance_percentage=0.0,
        )

        # Adapters should convert both to string representation
        assert status == COMPARISON_SUCCESS
        print(f"Boolean comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_sample_identical_timezone.py
РАЗМЕР: 3603 символов
==================================================

"""
Test timestamp with timezone comparison between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS


class TestTimestampWithTimezone:
    """Tests for timestamp with timezone comparison"""
    
    @pytest.fixture(autouse=True)
    def setup_timestamp_data(self, oracle_engine, postgres_engine):
        """Setup timestamp test data"""
        # Oracle
        with oracle_engine.begin() as conn:
            conn.execute(text("""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE test_timestamps CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text("""
                CREATE TABLE test_timestamps (
                    id NUMBER PRIMARY KEY,
                    created_at TIMESTAMP NOT NULL,
                    updated_at TIMESTAMP,
                    description VARCHAR2(100)
                )
            """))
            
            # Insert test data (implicitly in +05:00)
            conn.execute(text("""
                INSERT INTO test_timestamps (id, created_at, updated_at, description) VALUES
                (1, TIMESTAMP '2024-01-01 15:00:00', TIMESTAMP '2024-01-01 15:00:00', 'First record'),
                (2, TIMESTAMP '2024-01-02 15:30:00', TIMESTAMP '2024-01-02 15:30:00', 'Second record'),
                (3, TIMESTAMP '2024-01-03 10:45:00', TIMESTAMP '2024-01-03 10:45:00', 'Third record')
            """))
        
        # PostgreSQL
        with postgres_engine.begin() as conn:
            conn.execute(text("DROP TABLE IF EXISTS test_timestamps CASCADE"))
            
            conn.execute(text("""
                CREATE TABLE test_timestamps (
                    id INTEGER PRIMARY KEY,
                    created_at TIMESTAMPTZ NOT NULL,
                    updated_at TIMESTAMPTZ,
                    description TEXT
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_timestamps (id, created_at, updated_at, description) VALUES
                (1, '2024-01-01 10:00:00 +00:00', '2024-01-01 10:00:00 +00:00', 'First record'),
                (2, '2024-01-02 11:30:00 +01:00', '2024-01-02 11:30:00 +01:00', 'Second record'),
                (3, '2024-01-03 14:45:00 +09:00', '2024-01-03 14:45:00 +09:00', 'Third record')
            """))
        
        yield

    def test_timestamp_with_timezone(self, oracle_engine, postgres_engine):
        """
        Compare timestamp with timezone between Oracle and PostgreSQL.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="+05:00",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_timestamps", "test"),
            target_table=DataReference("test_timestamps", "test"),
            date_column="created_at",
            update_column="updated_at",
            date_range=("2024-01-01", "2024-01-31"),
            tolerance_percentage=0.0,
            exclude_recent_hours=24,
        )
        
        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"Timestamp with timezone comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/cross_db/oracle_postgres/test_ora_pg_sample_identical_null.py
РАЗМЕР: 3670 символов
==================================================

"""
Test NULL values comparison between Oracle and PostgreSQL.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestNullValuesComparison:
    """Tests for NULL values comparison"""
    
    @pytest.fixture(autouse=True)
    def setup_null_data(self, oracle_engine, postgres_engine):
        """Setup NULL test data"""
        
        table_name = "test_edge_nulls"
        
        # Oracle
        with oracle_engine.begin() as conn:
            conn.execute(text(f"""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE {table_name} CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id NUMBER PRIMARY KEY,
                    nullable_string VARCHAR2(100),
                    nullable_number NUMBER,
                    nullable_date DATE,
                    created_at DATE NOT NULL
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, nullable_string, nullable_number, nullable_date, created_at) VALUES
                (1, NULL, NULL, NULL, DATE '2024-01-01'),
                (2, 'Some text', 123, DATE '2024-01-02', DATE '2024-01-02'),
                (3, NULL, 456, NULL, DATE '2024-01-03')
            """))
        
        # PostgreSQL
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
            
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id INTEGER PRIMARY KEY,
                    nullable_string TEXT,
                    nullable_number INTEGER,
                    nullable_date DATE,
                    created_at DATE NOT NULL
                )
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name} (id, nullable_string, nullable_number, nullable_date, created_at) VALUES
                (1, NULL, NULL, NULL, '2024-01-01'),
                (2, 'Some text', 123, '2024-01-02', '2024-01-02'),
                (3, NULL, 456, NULL, '2024-01-03')
            """))
        
        yield
        
        # Cleanup
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text(f"DROP TABLE {table_name} CASCADE CONSTRAINTS"))
            except:
                pass
        
        with postgres_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))

    def test_null_values_comparison(self, oracle_engine, postgres_engine):
        """
        Compare tables with NULL values in different columns.
        """
        table_name = "test_edge_nulls"
        
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(table_name, "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-05"),
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        print(f"NULL values comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/docker/docker-compose.yml
РАЗМЕР: 1552 символов
==================================================

services:
  postgres:
    image: postgres:15.4
    container_name: postgres
    environment:
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_pass
      POSTGRES_DB: test_db
    ports:
      - "5433:5432"
    volumes:
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    mem_limit: 512m
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U test_user -d test_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  oracle:
    image: container-registry.oracle.com/database/free:23.26.0.0-lite
    container_name: oracle
    platform: linux/amd64
    ports:
      - "1521:1521"
    environment:
      ORACLE_PWD: test_pass
      ORACLE_PDB: test_db
    shm_size: 2g
    volumes:
      - ./oracle/init.sql:/opt/oracle/scripts/startup/init.sql
    healthcheck:
      test: ["CMD-SHELL", "/opt/oracle/checkDBStatus.sh"]
      interval: 30s
      timeout: 10s
      retries: 15
      start_period: 120s
    restart: unless-stopped

  clickhouse:
    image: clickhouse/clickhouse-server:23.8-alpine
    container_name: clickhouse
    environment:
      CLICKHOUSE_DB: test
      CLICKHOUSE_USER: test_user
      CLICKHOUSE_PASSWORD: test_pass
    ports:
      - "8123:8123" 
      - "9000:9000"
    volumes:
      - ./clickhouse/init.sql:/docker-entrypoint-initdb.d/init.sql
    mem_limit: 2g
    memswap_limit: 4g
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 3


==================================================
ФАЙЛ: tests/integration/docker/clickhouse/init.sql
РАЗМЕР: 118 символов
==================================================

USE test;

CREATE TABLE imalive (
    id UInt32
)
ENGINE = MergeTree()
ORDER BY id;

INSERT INTO imalive(id)
select 1;


==================================================
ФАЙЛ: tests/integration/docker/oracle/init.sql
РАЗМЕР: 308 символов
==================================================

--

ALTER SESSION SET CONTAINER = test_db;

CREATE USER test IDENTIFIED BY test_pass;
GRANT CONNECT, RESOURCE TO test;
ALTER USER test QUOTA UNLIMITED ON system;
ALTER SESSION set current_schema = test;



CREATE TABLE imalive (
    id          NUMBER
);

insert into imalive(id)
select 1 from dual;

commit;


==================================================
ФАЙЛ: tests/integration/docker/postgres/init.sql
РАЗМЕР: 213 символов
==================================================


CREATE SCHEMA IF NOT EXISTS test;

SET search_path TO test;

ALTER USER test_user SET SEARCH_PATH TO test, public;


CREATE TABLE imalive (
    id         INTEGER
);


INSERT INTO imalive (id) 
select 1;

commit;


==================================================
ФАЙЛ: tests/integration/self_db/clickhouse/test_clickhouse_sample_table_vs_table.py
РАЗМЕР: 3158 символов
==================================================

"""
Self-comparison test for ClickHouse table vs table.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestClickHouseTableVsTable:
    """Self-comparison tests within ClickHouse database"""
    
    @pytest.fixture(autouse=True)
    def setup_table_vs_table_data(self, clickhouse_engine):
        """Setup test data for ClickHouse table vs table comparison"""
        
        table_name_main = "test_self_ch_table_main"
        table_name_copy = "test_self_ch_table_copy"
        
        with clickhouse_engine.begin() as conn:
            # Clean up
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name_main}"))
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name_copy}"))
            
            # Create main table
            conn.execute(text(f"""
                CREATE TABLE {table_name_main} (
                    id UInt32,
                    value String,
                    created_at Date
                )
                ENGINE = MergeTree()
                ORDER BY id
            """))
            
            # Insert data
            conn.execute(text(f"""
                INSERT INTO {table_name_main} VALUES
                (1, 'Value A', '2024-01-01'),
                (2, 'Value B', '2024-01-02'),
                (3, 'Value C', '2024-01-03')
            """))
            
            # Create copy table (identical structure, same data)
            conn.execute(text(f"""
                CREATE TABLE {table_name_copy} (
                    id UInt32,
                    value String,
                    created_at Date
                )
                ENGINE = MergeTree()
                ORDER BY id
            """))
            
            conn.execute(text(f"""
                INSERT INTO {table_name_copy} 
                SELECT * FROM {table_name_main}
            """))
        
        yield
        
        # Cleanup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name_main}"))
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name_copy}"))

    def test_clickhouse_table_vs_table(self, clickhouse_engine):
        """
        Test comparison between two identical ClickHouse tables.
        """
        table_name_main = "test_self_ch_table_main"
        table_name_copy = "test_self_ch_table_copy"
        
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=clickhouse_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name_main, "test"),
            target_table=DataReference(table_name_copy, "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-04"),
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"ClickHouse table vs table comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/self_db/clickhouse/test_clickhouse_sample.py
РАЗМЕР: 3014 символов
==================================================

"""
Self-comparison test for ClickHouse table vs view.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS

class TestClickHouseTableVsView:
    """Self-comparison tests for ClickHouse table vs view"""
    
    @pytest.fixture(autouse=True)
    def setup_table_vs_view_data(self, clickhouse_engine):
        """Setup test data for ClickHouse table vs view comparison"""
        
        table_name = "test_self_ch_table_view_main"
        view_name = "v_test_self_ch_table_view"
        
        with clickhouse_engine.begin() as conn:
            # Clean up
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
            conn.execute(text(f"DROP VIEW IF EXISTS {view_name}"))
            
            # Create main table
            conn.execute(text(f"""
                CREATE TABLE {table_name} (
                    id UInt32,
                    name String,
                    price Decimal(10,2),
                    created_at Date,
                    internal_flag UInt8
                )
                ENGINE = MergeTree()
                ORDER BY id
            """))
            
            # Insert data
            conn.execute(text(f"""
                INSERT INTO {table_name} VALUES
                (1, 'Product A', 99.99, '2024-01-01', 1),
                (2, 'Product B', 149.50, '2024-01-02', 0),
                (3, 'Product C', 199.00, '2024-01-03', 1)
            """))
            
            # Create a view
            conn.execute(text(f"""
                CREATE VIEW {view_name} AS
                SELECT 
                    id,
                    name,
                    price,
                    created_at
                FROM {table_name}
            """))
        
        yield
        
        # Cleanup
        with clickhouse_engine.begin() as conn:
            conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
            conn.execute(text(f"DROP VIEW IF EXISTS {view_name}"))

    def test_clickhouse_table_vs_view(self, clickhouse_engine):
        """
        Test comparison between ClickHouse table and view.
        """
        table_name = "test_self_ch_table_view_main"
        view_name = "v_test_self_ch_table_view"
        
        comparator = DataQualityComparator(
            source_engine=clickhouse_engine,
            target_engine=clickhouse_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference(table_name, "test"),
            target_table=DataReference(view_name, "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-04"),
            include_columns=["id", "name", "price", "created_at"],
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        print(f"ClickHouse table vs view comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/self_db/oracle/test_oracle_sample_identical_complex_data_types.py
РАЗМЕР: 6109 символов
==================================================

"""
Test Oracle self-comparison with complex data types.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS, COMPARISON_SKIPPED


class TestOracleComplexDataTypes:
    """
    Tests for Oracle self-comparison with various data types.
    """
    
    @pytest.fixture(autouse=True)
    def setup_oracle_complex_data(self, oracle_engine):
        """Setup Oracle test data with complex types for self-comparison"""
        with oracle_engine.begin() as conn:
            conn.execute(text("""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE test_oracle_complex CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text("""
                CREATE TABLE test_oracle_complex (
                    id               NUMBER PRIMARY KEY,
                    varchar_col      VARCHAR2(200),
                    number_col       NUMBER(10,3),
                    date_col         DATE,
                    timestamp_col    TIMESTAMP,
                    timestamp_tz_col TIMESTAMP WITH TIME ZONE,
                    interval_col     INTERVAL DAY TO SECOND,
                    raw_col          RAW(50),
                    clob_col         CLOB,
                    created_at       DATE NOT NULL
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_oracle_complex (
                    id, varchar_col, number_col, date_col, timestamp_col,
                    timestamp_tz_col, interval_col, raw_col, clob_col, created_at
                ) VALUES
                (1, 'Standard text', 123.456, 
                 DATE '2024-01-01', 
                 TIMESTAMP '2024-01-01 10:30:45.123456',
                 TIMESTAMP '2024-01-01 10:30:45.123456 +00:00',
                 INTERVAL '1 2:30:45' DAY TO SECOND,
                 HEXTORAW('414243'),
                 'This is a CLOB text with multiple lines
                 and special characters: !@#$%^&*()',
                 DATE '2024-01-01'),
                 
                (2, NULL, 789.012,
                 DATE '2024-01-02',
                 TIMESTAMP '2024-01-02 14:20:30.987654',
                 TIMESTAMP '2024-01-02 14:20:30.987654 +05:00',
                 INTERVAL '0 6:15:30' DAY TO SECOND,
                 HEXTORAW('444546'),
                 'Another CLOB content',
                 DATE '2024-01-02'),
                 
                (3, 'Text with emoji 😀🚀📊', 0.001,
                 DATE '2024-01-03',
                 TIMESTAMP '2024-01-03 09:15:20.555555',
                 TIMESTAMP '2024-01-03 09:15:20.555555 -08:00',
                 INTERVAL '2 12:00:00' DAY TO SECOND,
                 NULL,
                 NULL,
                 DATE '2024-01-03')
            """))
        
        yield
        
        # Cleanup
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text("DROP TABLE test_oracle_complex CASCADE CONSTRAINTS"))
            except:
                pass

    def test_oracle_complex_types_self_comparison(self, oracle_engine):
        """
        Compare Oracle table with itself containing complex data types.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_oracle_complex", "test"),
            target_table=DataReference("test_oracle_complex", "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-03"),
            exclude_columns=["raw_col"],  # Exclude RAW columns as they might not compare well
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"Oracle complex types self-comparison passed: {stats.final_score:.2f}%")

    def test_oracle_with_column_exclusions(self, oracle_engine):
        """
        Test Oracle self-comparison with excluded columns.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_oracle_complex", "test"),
            target_table=DataReference("test_oracle_complex", "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-03"),
            exclude_columns=["raw_col", "clob_col", "interval_col"],  # Exclude problematic columns
            include_columns=["id", "varchar_col", "number_col", "date_col"],  # Include specific columns
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        print(f"Oracle with column exclusions passed: {stats.final_score:.2f}%")

    def test_oracle_empty_date_range(self, oracle_engine):
        """
        Test Oracle self-comparison with empty date range.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_oracle_complex", "test"),
            target_table=DataReference("test_oracle_complex", "test"),
            date_column="created_at",
            date_range=("2025-01-01", "2025-01-31"),  # Future date range, should be empty
            tolerance_percentage=0.0,
        )

        # Should be skipped due to empty result
        assert status == COMPARISON_SKIPPED
        print(f"Oracle empty date range test passed: No data to compare")


==================================================
ФАЙЛ: tests/integration/self_db/oracle/test_oracle_sample.py
РАЗМЕР: 4472 символов
==================================================

"""
Test Oracle self-comparison with identical data.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS


class TestOracleSelfComparison:
    """
    Tests comparing Oracle with itself (same engine).
    """
    
    @pytest.fixture(autouse=True)
    def setup_oracle_data(self, oracle_engine):
        """Setup Oracle test data for self-comparison"""
        with oracle_engine.begin() as conn:
            conn.execute(text("""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE test_oracle_self CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text("""
                CREATE TABLE test_oracle_self (
                    id          NUMBER PRIMARY KEY,
                    name        VARCHAR2(100) NOT NULL,
                    amount      NUMBER(10,2),
                    created_at  DATE NOT NULL,
                    updated_at  TIMESTAMP NOT NULL
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_oracle_self (id, name, amount, created_at, updated_at) VALUES
                (1, 'Product A', 100.50, DATE '2024-01-01', TIMESTAMP '2024-01-01 10:00:00'),
                (2, 'Product B', 250.75, DATE '2024-01-02', TIMESTAMP '2024-01-02 11:30:00'),
                (3, 'Product C', 99.99, DATE '2024-01-03', TIMESTAMP '2024-01-03 14:45:00')
            """))
        
        yield
        
        # Cleanup
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text("DROP TABLE test_oracle_self CASCADE CONSTRAINTS"))
            except:
                pass

    def test_oracle_self_comparison_identical(self, oracle_engine):
        """
        Compare identical tables within same Oracle database.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_oracle_self", "test"),
            target_table=DataReference("test_oracle_self", "test"),
            date_column="created_at",
            update_column="updated_at",
            date_range=("2024-01-01", "2024-01-03"),
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"Oracle self-comparison passed: {stats.final_score:.2f}%")

    def test_oracle_table_vs_view(self, oracle_engine):
        """
        Compare Oracle table with view on the same data.
        """
        # Create a view for testing
        with oracle_engine.begin() as conn:
            conn.execute(text("""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP VIEW v_test_oracle_self CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text("""
                CREATE VIEW v_test_oracle_self AS
                SELECT id, name, amount, created_at, updated_at
                FROM test.test_oracle_self
            """))

        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_oracle_self", "test"),  # таблица
            target_table=DataReference("v_test_oracle_self", "test"),  # вьюха
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-03"),
            tolerance_percentage=0.0,
        )

        # Clean up the view
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text("DROP VIEW v_test_oracle_self CASCADE CONSTRAINTS"))
            except:
                pass

        assert status == COMPARISON_SUCCESS
        print(f"Oracle table vs view comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: tests/integration/self_db/oracle/test_oracle_sample_data_types.py
РАЗМЕР: 6109 символов
==================================================

"""
Test Oracle self-comparison with complex data types.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS, COMPARISON_SKIPPED


class TestOracleComplexDataTypes:
    """
    Tests for Oracle self-comparison with various data types.
    """
    
    @pytest.fixture(autouse=True)
    def setup_oracle_complex_data(self, oracle_engine):
        """Setup Oracle test data with complex types for self-comparison"""
        with oracle_engine.begin() as conn:
            conn.execute(text("""
                BEGIN
                    EXECUTE IMMEDIATE 'DROP TABLE test_oracle_complex CASCADE CONSTRAINTS';
                EXCEPTION
                    WHEN OTHERS THEN
                        IF SQLCODE != -942 THEN
                            RAISE;
                        END IF;
                END;
            """))
            
            conn.execute(text("""
                CREATE TABLE test_oracle_complex (
                    id               NUMBER PRIMARY KEY,
                    varchar_col      VARCHAR2(200),
                    number_col       NUMBER(10,3),
                    date_col         DATE,
                    timestamp_col    TIMESTAMP,
                    timestamp_tz_col TIMESTAMP WITH TIME ZONE,
                    interval_col     INTERVAL DAY TO SECOND,
                    raw_col          RAW(50),
                    clob_col         CLOB,
                    created_at       DATE NOT NULL
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_oracle_complex (
                    id, varchar_col, number_col, date_col, timestamp_col,
                    timestamp_tz_col, interval_col, raw_col, clob_col, created_at
                ) VALUES
                (1, 'Standard text', 123.456, 
                 DATE '2024-01-01', 
                 TIMESTAMP '2024-01-01 10:30:45.123456',
                 TIMESTAMP '2024-01-01 10:30:45.123456 +00:00',
                 INTERVAL '1 2:30:45' DAY TO SECOND,
                 HEXTORAW('414243'),
                 'This is a CLOB text with multiple lines
                 and special characters: !@#$%^&*()',
                 DATE '2024-01-01'),
                 
                (2, NULL, 789.012,
                 DATE '2024-01-02',
                 TIMESTAMP '2024-01-02 14:20:30.987654',
                 TIMESTAMP '2024-01-02 14:20:30.987654 +05:00',
                 INTERVAL '0 6:15:30' DAY TO SECOND,
                 HEXTORAW('444546'),
                 'Another CLOB content',
                 DATE '2024-01-02'),
                 
                (3, 'Text with emoji 😀🚀📊', 0.001,
                 DATE '2024-01-03',
                 TIMESTAMP '2024-01-03 09:15:20.555555',
                 TIMESTAMP '2024-01-03 09:15:20.555555 -08:00',
                 INTERVAL '2 12:00:00' DAY TO SECOND,
                 NULL,
                 NULL,
                 DATE '2024-01-03')
            """))
        
        yield
        
        # Cleanup
        with oracle_engine.begin() as conn:
            try:
                conn.execute(text("DROP TABLE test_oracle_complex CASCADE CONSTRAINTS"))
            except:
                pass

    def test_oracle_complex_types_self_comparison(self, oracle_engine):
        """
        Compare Oracle table with itself containing complex data types.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_oracle_complex", "test"),
            target_table=DataReference("test_oracle_complex", "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-03"),
            exclude_columns=["raw_col"],  # Exclude RAW columns as they might not compare well
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"Oracle complex types self-comparison passed: {stats.final_score:.2f}%")

    def test_oracle_with_column_exclusions(self, oracle_engine):
        """
        Test Oracle self-comparison with excluded columns.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_oracle_complex", "test"),
            target_table=DataReference("test_oracle_complex", "test"),
            date_column="created_at",
            date_range=("2024-01-01", "2024-01-03"),
            exclude_columns=["raw_col", "clob_col", "interval_col"],  # Exclude problematic columns
            include_columns=["id", "varchar_col", "number_col", "date_col"],  # Include specific columns
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        print(f"Oracle with column exclusions passed: {stats.final_score:.2f}%")

    def test_oracle_empty_date_range(self, oracle_engine):
        """
        Test Oracle self-comparison with empty date range.
        """
        comparator = DataQualityComparator(
            source_engine=oracle_engine,
            target_engine=oracle_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_oracle_complex", "test"),
            target_table=DataReference("test_oracle_complex", "test"),
            date_column="created_at",
            date_range=("2025-01-01", "2025-01-31"),  # Future date range, should be empty
            tolerance_percentage=0.0,
        )

        # Should be skipped due to empty result
        assert status == COMPARISON_SKIPPED
        print(f"Oracle empty date range test passed: No data to compare")


==================================================
ФАЙЛ: tests/integration/self_db/postgres/test_postgres_sample.py
РАЗМЕР: 2609 символов
==================================================

"""
Test PostgreSQL self-comparison with identical data.
"""

import pytest
from sqlalchemy import text
from xoverrr.core import DataQualityComparator, DataReference
from xoverrr.constants import COMPARISON_SUCCESS


class TestPostgresSelfComparison:
    """
    Tests comparing PostgreSQL with itself (same engine).
    """
    
    @pytest.fixture(autouse=True)
    def setup_postgres_data(self, postgres_engine):
        """Setup PostgreSQL test data for self-comparison"""
        with postgres_engine.begin() as conn:
            conn.execute(text("DROP TABLE IF EXISTS test_custom_data2 CASCADE"))
            
            conn.execute(text("""
                CREATE TABLE test_custom_data2 (
                    id          INTEGER PRIMARY KEY,
                    name        TEXT NOT NULL,
                    created_at  DATE NOT NULL,
                    updated_at  TIMESTAMP NOT NULL
                )
            """))
            
            conn.execute(text("""
                INSERT INTO test_custom_data2 (id, name, created_at, updated_at) VALUES
                (1, 'Alice',   '2024-01-01', '2024-01-01 10:00:00'),
                (2, 'Robert',  '2024-01-02', '2024-01-02 11:00:00'),
                (3, 'Charlie', '2024-01-03', '2024-01-03 12:00:00')
            """))

        """Setup view for PostgreSQL self-comparison"""
        with postgres_engine.begin() as conn:
            # Create a view
            conn.execute(text("DROP VIEW IF EXISTS vtest_custom_data2 CASCADE"))
            conn.execute(text("""
                CREATE VIEW vtest_custom_data2 AS
                SELECT id, name, created_at, updated_at
                FROM test_custom_data2
            """))
        
        yield

    def test_postgres_self_comparison_identical(self, postgres_engine):
        """
        Compare identical tables within same PostgreSQL database.
        """
        comparator = DataQualityComparator(
            source_engine=postgres_engine,
            target_engine=postgres_engine,
            timezone="UTC",
        )

        status, report, stats, details = comparator.compare_sample(
            source_table=DataReference("test_custom_data2", "test"),
            target_table=DataReference("test_custom_data2", "test"),
            date_column="created_at",
            update_column="updated_at",
            date_range=("2024-01-01", "2024-01-03"),
            tolerance_percentage=0.0,
        )

        assert status == COMPARISON_SUCCESS
        assert stats.final_diff_score == 0.0
        print(f"PostgreSQL self-comparison passed: {stats.final_score:.2f}%")


==================================================
ФАЙЛ: src/xoverrr/models.py
РАЗМЕР: 1591 символов
==================================================

from dataclasses import dataclass
from enum import Enum, auto
from typing import Optional
import re
from sqlalchemy.engine import Engine

class ObjectType(Enum):
    """Types of database objects"""
    TABLE = auto()
    VIEW = auto()
    MATERIALIZED_VIEW = auto()
    UNKNOWN = auto()

class DBMSType(Enum):
    """Supported database management systems"""
    ORACLE = auto()
    POSTGRESQL = auto()
    CLICKHOUSE = auto()

    @classmethod
    def from_engine(cls, engine: Engine) -> 'DBMSType':
        """Infer DBMS type from SQLAlchemy engine"""
        dialect = engine.dialect.name.lower()
        if dialect == 'oracle':
            return cls.ORACLE
        elif dialect in ('postgresql', 'postgres'):
            return cls.POSTGRESQL
        elif dialect == 'clickhouse':
            return cls.CLICKHOUSE
        raise ValueError(f"Unsupported engine dialect: {dialect}")


@dataclass(frozen=True)
class DataReference:
    """Immutable reference to a database object"""
    name: str
    schema: Optional[str] = None

    def __post_init__(self):
        self._validate()

    def _validate(self):
        """Validate table reference parameters"""
        if not re.match(r'^[a-zA-Z0-9_]+$', self.name):
            raise ValueError(f"Invalid table name: {self.name}")
        if self.schema and not re.match(r'^[a-zA-Z0-9_]+$', self.schema):
            raise ValueError(f"Invalid schema name: {self.schema}")

    @property
    def full_name(self) -> str:
        """Get fully qualified object name"""
        return f"{self.schema}.{self.name}" if self.schema else self.name


==================================================
ФАЙЛ: src/xoverrr/constants.py
РАЗМЕР: 461 символов
==================================================

# Date and time formats
DATE_FORMAT = "%Y-%m-%d"
DATETIME_FORMAT = f"{DATE_FORMAT} %H:%M:%S"

# Default values
NULL_REPLACEMENT = "N/A"
DEFAULT_MAX_EXAMPLES = 3
DEFAULT_MAX_SAMPLE_SIZE_GB = 3  # Max size of dataframe to compare

# SQL patterns
RESERVED_WORDS = ['date', 'comment', 'file', 'number', 'mode', 'successful']

DEFAULT_TZ = 'UTC'

# Comparison result statuses
COMPARISON_SUCCESS = 'success'
COMPARISON_FAILED = 'failed'
COMPARISON_SKIPPED = 'skipped'


==================================================
ФАЙЛ: src/xoverrr/__init__.py
РАЗМЕР: 319 символов
==================================================


from .core import DataQualityComparator, DataReference
from .constants import (
    COMPARISON_SUCCESS,
    COMPARISON_FAILED,
    COMPARISON_SKIPPED,
)

__all__ = [
    "DataQualityComparator",
    "DataReference",
    "COMPARISON_SUCCESS",
    "COMPARISON_FAILED",
    "COMPARISON_SKIPPED",
]

__version__ = "1.1.5"



==================================================
ФАЙЛ: src/xoverrr/core.py
РАЗМЕР: 26037 символов
==================================================


import sys
from enum import Enum, auto
from typing import Optional, List, Dict, Callable, Union, Tuple, Any
import pandas as pd
from sqlalchemy.engine import Engine
from .models import (
    DBMSType,
    DataReference,
    ObjectType
)

from .logger import app_logger

from .adapters.oracle import OracleAdapter
from .adapters.postgres import PostgresAdapter
from .adapters.clickhouse import ClickHouseAdapter
from .adapters.base import BaseDatabaseAdapter

from . import constants as ct

from .exceptions import (
    MetadataError,
    DQCompareException
)
from .utils import (
    prepare_dataframe,
    compare_dataframes,
    clean_recently_changed_data,
    generate_comparison_sample_report,
    generate_comparison_count_report,
    cross_fill_missing_dates,
    validate_dataframe_size,
    ComparisonStats,
    ComparisonDiffDetails
)




class DataQualityComparator:
    """
    Main comparison class implementing data quality checks between databases.
    """

    def __init__(
        self,
        source_engine: Engine,
        target_engine: Engine,
        default_exclude_recent_hours: Optional[int] = 24,
        timezone: str = ct.DEFAULT_TZ
    ):
        self.source_engine = source_engine
        self.target_engine = target_engine
        self.source_db_type = DBMSType.from_engine(source_engine)
        self.target_db_type = DBMSType.from_engine(target_engine)
        self.default_exclude_recent_hours = default_exclude_recent_hours
        self.timezone = timezone

        self.adapters = {
            DBMSType.ORACLE: OracleAdapter(),
            DBMSType.POSTGRESQL: PostgresAdapter(),
            DBMSType.CLICKHOUSE: ClickHouseAdapter(),
        }
        self._reset_stats()
        app_logger.info('start')

    def reset_stats(self):
        self._reset_stats()

    def _reset_stats(self):
        self.comparison_stats = {
            'compared': 0,
            ct.COMPARISON_SUCCESS: 0,
            ct.COMPARISON_FAILED: 0,
            ct.COMPARISON_SKIPPED: 0,
            'tables_success' : set(),
            'tables_failed' : set(),
            'tables_skipped': set(),
            'start_time': pd.Timestamp.now().strftime(ct.DATETIME_FORMAT),
            'end_time': None
        }

    def _update_stats(self, status: str, source_table:DataReference):
        """Update comparison statistics"""
        self.comparison_stats[status] += 1
        self.comparison_stats['end_time'] = pd.Timestamp.now().strftime(ct.DATETIME_FORMAT)
        if source_table:
            match status:
                case ct.COMPARISON_SUCCESS:
                    self.comparison_stats['tables_success'].add(source_table.full_name)
                case ct.COMPARISON_FAILED:
                    self.comparison_stats['tables_failed'].add(source_table.full_name)
                case ct.COMPARISON_SKIPPED:
                    self.comparison_stats['tables_skipped'].add(source_table.full_name)

    def compare_counts(
        self,
        source_table: DataReference,
        target_table: DataReference,
        date_column: Optional[str] = None,
        date_range: Optional[Tuple[str, str]] = None,
        tolerance_percentage: float = 0.0,
        max_examples: Optional[int] = ct.DEFAULT_MAX_EXAMPLES
    ) -> Tuple[str, Optional[ComparisonStats], Optional[ComparisonDiffDetails]]:

        self._validate_inputs(source_table, target_table)

        start_date, end_date = date_range or (None, None)

        try:
            self.comparison_stats['compared'] += 1


            status, report, stats, details = self._compare_counts(
                    source_table, target_table, date_column, start_date, end_date,
                    tolerance_percentage, max_examples
            )

            self._update_stats(status, source_table)
            return status, report, stats, details

        except Exception as e:
            app_logger.exception(f"Count comparison failed: {str(e)}")
            status = ct.COMPARISON_FAILED
            self._update_stats(status, source_table)
            return status, None, None, None

    def compare_sample(
        self,
        source_table: DataReference,
        target_table: DataReference,
        date_column: Optional[str] = None,
        update_column: Optional[str] = None,
        date_range: Optional[Tuple[str, str]] = None,
        exclude_columns: Optional[List[str]] = None,
        include_columns: Optional[List[str]] = None,
        custom_primary_key: Optional[List[str]] = None,
        tolerance_percentage: float = 0.0,
        exclude_recent_hours: Optional[int] = None,
        max_examples: Optional[int] = ct.DEFAULT_MAX_EXAMPLES
    ) -> Tuple[str, str, Optional[ComparisonStats], Optional[ComparisonDiffDetails]]:
        """
        Compare data from custom queries with specified key columns

        Parameters:
            source_table: `DataReference` 
                source table to compare
            target_table: `DataReference`
                target table to compare
            custom_primary_key : `List[str]`
                List of primary key columns for comparison.
            exclude_columns : `Optional[List[str]] = None` 
                Columns to exclude from comparison.
            include_columns : `Optional[List[str]] = None` 
                Columns to include from comparison (default all cols)
            tolerance_percentage : `float` 
                Tolerance percentage for discrepancies.
            max_examples 
                Maximum number of discrepancy examples per column
        """
        self._validate_inputs(source_table, target_table)

        exclude_hours = exclude_recent_hours or self.default_exclude_recent_hours

        start_date, end_date = date_range or (None, None)
        exclude_cols = exclude_columns or []
        custom_keys = custom_primary_key
        include_cols = include_columns or []

        try:
            self.comparison_stats['compared'] += 1

            status, report, stats, details = self._compare_samples(
                    source_table, target_table, date_column, update_column,
                    start_date, end_date, exclude_cols,include_cols, 
                    custom_keys, tolerance_percentage, exclude_hours, max_examples
            )

            self._update_stats(status, source_table)
            return status, report, stats, details

        except Exception as e:
            app_logger.exception(f"Sample comparison failed: {str(e)}")
            status = ct.COMPARISON_FAILED
            self._update_stats(status, source_table)
            return status, None, None, None

    def _compare_counts(self, source_table: DataReference,
                        target_table: DataReference,
                        date_column: str,
                        start_date: Optional[str],
                        end_date: Optional[str],
                        tolerance_percentage:float,
                        max_examples:int) -> Tuple[str, str, Optional[ComparisonStats], Optional[ComparisonDiffDetails]]:

        try:
            source_adapter = self._get_adapter(self.source_db_type)
            target_adapter = self._get_adapter(self.target_db_type)

            source_query, source_params = source_adapter.build_count_query(
                source_table, date_column, start_date, end_date
            )
            source_counts = self._execute_query((source_query, source_params), self.source_engine, self.timezone)

            target_query, target_params = target_adapter.build_count_query(
                target_table, date_column, start_date, end_date
            )
            target_counts = self._execute_query((target_query, target_params), self.target_engine, self.timezone)


            source_counts_filled, target_counts_filled = cross_fill_missing_dates(source_counts, target_counts)

            merged = source_counts_filled.merge(target_counts_filled, on='dt')
            total_count_source = source_counts_filled['cnt'].sum()
            total_count_taget =  target_counts_filled['cnt'].sum()

            if (total_count_source, total_count_taget)  ==  (0,0):
                app_logger.warning('nothing to compare to you')
                status = ct.COMPARISON_SKIPPED
                return status, None, None, None

            else:

                result_diff_in_counters = abs(merged['cnt_x'] - merged['cnt_y']).sum()
                result_equal_in_counters = merged[['cnt_x', 'cnt_y']].min(axis=1).sum()

                discrepancies_counters_percentage = 100*result_diff_in_counters/(result_diff_in_counters+result_equal_in_counters)
                stats, details = compare_dataframes(source_df=source_counts_filled,
                                                    target_df=target_counts_filled,
                                                    key_columns=['dt'],
                                                    max_examples=max_examples)

                status = ct.COMPARISON_FAILED if discrepancies_counters_percentage > tolerance_percentage else ct.COMPARISON_SUCCESS

                report = generate_comparison_count_report(source_table.full_name,
                                                          target_table.full_name,
                                                          stats,
                                                          details,
                                                          total_count_source,
                                                          total_count_taget,
                                                          discrepancies_counters_percentage,
                                                          result_diff_in_counters,
                                                          result_equal_in_counters,
                                                          self.timezone,
                                                          source_query,
                                                          source_params,
                                                          target_query,
                                                          target_params
                                                        )

                return status, report, stats, details

        except Exception as e:
            app_logger.error(f"Count comparison failed: {str(e)}")
            raise

    def _compare_samples(
        self,
        source_table: DataReference,
        target_table: DataReference,
        date_column: str,
        update_column: str,
        start_date: Optional[str],
        end_date: Optional[str],
        exclude_columns: List[str],
        include_columns: List[str],
        custom_key_columns: Optional[List[str]],
        tolerance_percentage:float,
        exclude_recent_hours: Optional[int],
        max_examples:Optional[int]
    ) -> Tuple[str, str, Optional[ComparisonStats], Optional[ComparisonDiffDetails]]:

        try:
            source_object_type = self._get_object_type(source_table, self.source_engine)
            target_object_type = self._get_object_type(target_table, self.target_engine)
            app_logger.info(f'object type source: {source_object_type} vs target {target_object_type}')

            source_columns_meta = self._get_metadata_cols(source_table, self.source_engine)
            app_logger.info('source_columns meta:\n')
            app_logger.info(source_columns_meta.to_string(index=False))

            target_columns_meta = self._get_metadata_cols(target_table, self.target_engine)
            app_logger.info('target_columns meta:\n')
            app_logger.info(target_columns_meta.to_string(index=False))

            intersect = list(set(include_columns)&set(exclude_columns))
            if intersect:
                app_logger.warning(f'Intersection columns between Include and exclude: {",".join(intersect)}')
            
            key_columns = None

            if custom_key_columns:
                key_columns = custom_key_columns
                source_cols = source_columns_meta['column_name'].tolist()
                target_cols = target_columns_meta['column_name'].tolist()

                missing_in_source = [col for col in custom_key_columns if col not in source_cols]
                missing_in_target = [col for col in custom_key_columns if col not in target_cols]

                if missing_in_source:
                    raise MetadataError(f"Custom key columns missing in source: {missing_in_source}")
                if missing_in_target:
                    raise MetadataError(f"Custom key columns missing in target: {missing_in_target}")
            else:
                source_pk = self._get_metadata_pk(source_table, self.source_engine) \
                                         if source_object_type == ObjectType.TABLE else pd.DataFrame({'pk_column_name': []})
                target_pk = self._get_metadata_pk(target_table, self.target_engine) \
                                         if target_object_type == ObjectType.TABLE else pd.DataFrame({'pk_column_name': []})

                if source_pk['pk_column_name'].tolist() != target_pk['pk_column_name'].tolist():
                    app_logger.warning(f"Primary keys differ: source={source_pk['pk_column_name'].tolist()}, target={target_pk['pk_column_name'].tolist()}")
                key_columns = source_pk['pk_column_name'].tolist() or target_pk['pk_column_name'].tolist()
                if not key_columns:
                    raise MetadataError(f"Primary key not found in the source neither in the target and not provided") 

            if include_columns:
            
                if not set(include_columns) & set(key_columns):
                    app_logger.warning(f'The primary key was not included in the column list.\
                                       The key column was included in the resulting query automatically. PK:{key_columns}') 

                include_columns = list(set(include_columns + key_columns))

                source_columns_meta = source_columns_meta[
                    source_columns_meta['column_name'].isin(include_columns)
                ]
                target_columns_meta = target_columns_meta[
                    target_columns_meta['column_name'].isin(include_columns)
                ]
            
            if exclude_columns:

                if set(exclude_columns) & set(key_columns):
                    app_logger.warning(f'The primary key has been excluded from the column list.\
                                       However, the key column must be present in the resulting query.s PK:{key_columns}') 

                exclude_columns = list(set(exclude_columns) - set(key_columns))

                source_columns_meta = source_columns_meta[
                    ~source_columns_meta['column_name'].isin(exclude_columns)
                ]
                target_columns_meta = target_columns_meta[
                    ~target_columns_meta['column_name'].isin(exclude_columns)
                ]

            common_cols_df, source_only_cols, target_only_cols = self._analyze_columns_meta(source_columns_meta, target_columns_meta)
            common_cols = common_cols_df['column_name'].tolist()

            if not common_cols:
                raise MetadataError(f"No one column to compare, need to check tables or reduce the exclude_columns list: {','.join(exclude_columns)}")
            
            source_data, source_query, source_params = self._get_table_data(
                self.source_engine, source_table, source_columns_meta, common_cols,
                date_column, update_column, start_date, end_date, exclude_recent_hours
            )

            target_data, target_query, target_params = self._get_table_data(
                self.target_engine, target_table, target_columns_meta, common_cols,
                date_column, update_column, start_date, end_date, exclude_recent_hours
            )
            status = None
            #special case
            if target_data.empty and source_data.empty:
                status = ct.COMPARISON_SKIPPED
                return status, None, None, None
            elif source_data.empty or target_data.empty:
                raise DQCompareException(f"Nothing to compare, rows returned from source: {len(source_data)}, from target: {len(target_data)}")


            source_data = prepare_dataframe(source_data)
            target_data = prepare_dataframe(target_data)
            if update_column and exclude_recent_hours:
                source_data, target_data = clean_recently_changed_data(source_data, target_data, key_columns)


            stats, details = compare_dataframes(
                source_data, target_data,
                key_columns, max_examples
            )

            if stats:
                details.skipped_source_columns = source_only_cols
                details.skipped_target_columns = target_only_cols

                report = generate_comparison_sample_report(source_table.full_name,
                                                            target_table.full_name,
                                                            stats,
                                                            details,
                                                            self.timezone,
                                                            source_query,
                                                            source_params,
                                                            target_query,
                                                            target_params
                                                            )
                status = ct.COMPARISON_FAILED if stats.final_diff_score > tolerance_percentage else ct.COMPARISON_SUCCESS
                return status, report, stats, details
            else:
                status = ct.COMPARISON_SKIPPED
                return status, None, None, None

        except Exception as e:
            app_logger.error(f"Sample comparison failed: {str(e)}")
            raise

    def compare_custom_query(
        self,
        source_query: str,
        source_params: Tuple[str, Dict],
        target_query: str,
        target_params: Tuple[str, Dict],
        custom_primary_key: List[str],
        exclude_columns: Optional[List[str]] = None,
        tolerance_percentage: float = 0.0,
        max_examples:Optional[int] = ct.DEFAULT_MAX_EXAMPLES
    ) -> Tuple[str, str, Optional[ComparisonStats], Optional[ComparisonDiffDetails]]:
        """
        Compare data from custom queries with specified key columns

        Parameters:
            source_query : Union[str, Tuple[str, Dict]]  
                Source query (can be string or tuple with query and params).
            target_query : Union[str, Tuple[str, Dict]]
                Target query (can be string or tuple with query and params).
            custom_primary_key : List[str]
                List of primary key columns for comparison.
            exclude_columns : Optional[List[str]] = None 
                Columns to exclude from comparison.
            tolerance_percentage : float 
                Tolerance percentage for discrepancies.
            max_examples: int
                Maximum number of discrepancy examples per column 
                
        Returns:
        ----------
            Tuple[str, Optional[ComparisonStats], Optional[ComparisonDiffDetails]]
        """
        source_engine = self.source_engine
        target_engine = self.target_engine
        timezone = self.timezone

        try:
            self.comparison_stats['compared'] += 1

            # Execute queries
            source_data = self._execute_query((source_query,source_params), source_engine, timezone)
            target_data = self._execute_query((target_query,target_params), target_engine, timezone)
            app_logger.info('preparing source dataframe')
            source_data_prepared = prepare_dataframe(source_data)
            app_logger.info('preparing target dataframe')
            target_data_prepared = prepare_dataframe(target_data)

            # Exclude columns if specified
            exclude_cols = exclude_columns or []
            common_cols = [col for col in source_data_prepared.columns
                        if col in target_data_prepared.columns and col not in exclude_cols]

            source_data_filtered = source_data_prepared[common_cols]
            target_data_filtered = target_data_prepared[common_cols]
            if 'xrecently_changed' in common_cols:
                source_data_filtered, target_data_filtered = clean_recently_changed_data(source_data_filtered, target_data_filtered, custom_primary_key)
            # Compare dataframes
            stats, details = compare_dataframes(
                source_data_filtered, target_data_filtered, custom_primary_key, max_examples
            )

            if stats:
                report = generate_comparison_sample_report(None,
                                                           None,
                                                           stats,
                                                           details,
                                                           self.timezone,
                                                           source_query,
                                                           source_params,
                                                           target_query,
                                                           target_params
                                                          )
                status = ct.COMPARISON_FAILED if stats.final_diff_score > tolerance_percentage else ct.COMPARISON_SUCCESS
            else:
                status = ct.COMPARISON_SKIPPED


            self._update_stats(status, None)
            return status, report, stats, details

        except Exception as e:
            app_logger.exception("Custom query comparison failed")
            status = ct.COMPARISON_FAILED
            self._update_stats(status, None)
            return status, None, None, None
    def _get_metadata_cols(self, data_ref: DataReference, engine: Engine) -> pd.DataFrame:
        """Get metadata with proper source handling"""
        adapter = self._get_adapter(DBMSType.from_engine(engine))

        query, params = adapter.build_metadata_columns_query(data_ref)
        columns_meta = self._execute_query((query, params), engine)

        if columns_meta.empty:
            raise ValueError(f"Failed to get metadata for: {data_ref.full_name}")

        return columns_meta

    def _get_metadata_pk(self, data_ref: DataReference, engine: Engine) -> pd.DataFrame:
        """Get metadata with proper source handling
        """
        adapter = self._get_adapter(DBMSType.from_engine(engine))

        query, params = adapter.build_primary_key_query(data_ref)
        primary_key = self._execute_query((query, params), engine)

        return primary_key

    def _get_object_type(self, data_ref: DataReference, engine: Engine) -> pd.DataFrame:

        adapter = self._get_adapter(DBMSType.from_engine(engine))
        object_type = adapter.get_object_type(data_ref, engine)
        return object_type

    def _get_table_data(
        self,
        engine,
        data_ref: DataReference,
        metadata,
        columns: List[str],
        date_column: str,
        update_column: str,
        start_date: Optional[str],
        end_date: Optional[str],
        exclude_recent_hours: Optional[int]
    ) -> Tuple[pd.DataFrame, str, Dict] :
        """Retrieve and prepare table data"""
        db_type = DBMSType.from_engine(engine)
        adapter = self._get_adapter(db_type)
        app_logger.info(db_type)

        query, params = adapter.build_data_query_common(
            data_ref, columns, date_column, update_column,
            start_date, end_date, exclude_recent_hours
        )

        df = self._execute_query((query,params), engine, self.timezone)

        # Apply type conversions
        df = adapter.convert_types(df, metadata, self.timezone)

        return df, query, params

    def _get_adapter(self, db_type: DBMSType) -> BaseDatabaseAdapter:
        """Get adapter for specific DBMS"""
        try:
            return self.adapters[db_type]
        except KeyError:
            raise ValueError(f"No adapter available for {db_type}")

    def _execute_query(self, query: Union[str, Tuple[str, Dict]], engine: Engine, timezone: str = None) -> pd.DataFrame:
        """Execute SQL query using appropriate adapter"""
        db_type = DBMSType.from_engine(engine)
        adapter = self._get_adapter(db_type)
        df = adapter._execute_query(query, engine, timezone)
        validate_dataframe_size(df, ct.DEFAULT_MAX_SAMPLE_SIZE_GB)
        return df

    def _analyze_columns_meta(
        self,
        source_columns_meta: pd.DataFrame,
        target_columns_meta: pd.DataFrame
    ) -> tuple[pd.DataFrame, list, list]:
        """Find common columns between source and target and return unique columns for each"""

        source_columns = source_columns_meta['column_name'].tolist()
        target_columns = target_columns_meta['column_name'].tolist()

        common_columns = pd.merge(
            source_columns_meta, target_columns_meta,
            on='column_name', suffixes=('_source', '_target')
        )

        source_set = set(source_columns)
        target_set = set(target_columns)

        source_unique = list(source_set - target_set)
        target_unique = list(target_set - source_set)

        return common_columns, source_unique, target_unique

    def _validate_inputs(
        self,
        source: DataReference,
        target: DataReference
    ):
        """Validate input parameters"""
        if not isinstance(source, DataReference):
            raise TypeError("source must be a DataReference")
        if not isinstance(target, DataReference):
            raise TypeError("target must be a DataReference")


==================================================
ФАЙЛ: src/xoverrr/logger.py
РАЗМЕР: 361 символов
==================================================

import logging

app_logger = logging.getLogger(__name__)
app_logger.setLevel(logging.INFO)

formatter = logging.Formatter('%(asctime)s - %(levelname)s - xoverrr.%(module)s.%(funcName)s - %(message)s')

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
app_logger.addHandler(console_handler)


==================================================
ФАЙЛ: src/xoverrr/utils.py
РАЗМЕР: 25365 символов
==================================================

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple, defaultdict
from datetime import datetime

try:
    from .constants import NULL_REPLACEMENT, DEFAULT_MAX_EXAMPLES, DATETIME_FORMAT
    from .logger import app_logger
except ImportError:
    # for cases when used as standalone script
    from constants import NULL_REPLACEMENT, DEFAULT_MAX_EXAMPLES, DATETIME_FORMAT
    from logger import app_logger

from dataclasses import dataclass, field

@dataclass
class ComparisonStats:
    """Class for storing comparison statistics"""
    total_source_rows: int
    total_target_rows: int

    dup_source_rows: int
    dup_target_rows: int

    only_source_rows: int
    only_target_rows: int
    common_pk_rows: int
    total_matched_rows: int
    # percentages
    dup_source_percentage_rows: float
    dup_target_percentage_rows: float

    source_only_percentage_rows: float
    target_only_percentage_rows: float
    total_diff_percentage_rows : float
    #
    max_diff_percentage_cols : float
    median_diff_percentage_cols: float
    #
    final_diff_score: float
    final_score : float

@dataclass
class ComparisonDiffDetails:
    mismatches_per_column: pd.DataFrame
    discrepancies_per_col_examples: pd.DataFrame

    dup_source_keys_examples: tuple
    dup_target_keys_examples: tuple

    source_only_keys_examples: tuple
    target_only_keys_examples: tuple

    discrepant_data_examples:  pd.DataFrame
    common_attribute_columns: List[str]
    skipped_source_columns: List[str]= field(default_factory=list)
    skipped_target_columns: List[str]= field(default_factory=list)


def compare_dataframes_meta(
    df1: pd.DataFrame,
    df2: pd.DataFrame,
    primary_keys: List[str] = None
) -> List[str]:
    """
    Compare two pandas DataFrames and find common and different columns.

    Parameters:
    -----------
    df1, df2 : pd.DataFrame
        DataFrames to compare
    primary_keys : List[str], optional
        List of primary key columns to exclude from comparison

    Returns:
    --------
    - common_columns: List of common columns (ordered as in df1)
    """
    if primary_keys is None:
        primary_keys = []

    # Get all columns excluding primary keys
    df1_cols = [col for col in df1.columns if col not in primary_keys]
    df2_cols = [col for col in df2.columns if col not in primary_keys]

    # Convert to sets for efficient comparison
    df1_set = set(df1_cols)
    df2_set = set(df2_cols)

    # Find common columns (preserve order from df1)
    common_columns = [col for col in df1_cols if col in df2_set]

    return common_columns

def analyze_column_discrepancies(df, primary_key_columns, value_columns, common_keys_cnt, examples_count=3):

    metrics = {'max_pct' : 0.0, 'median_pct' : 0.0}
    diff_counters = defaultdict(int)
    diff_examples = {col: [] for col in value_columns}

    rows = list(df.itertuples(index=False))

    pk_indices = [df.columns.get_loc(col) for col in primary_key_columns]

    # scan through pairs
    for i in range(0, len(rows) - 1, 2):
        src_row = rows[i]
        trg_row = rows[i + 1]

        # for compound key use tuple, otherwise just value
        if len(pk_indices) > 1:
            pk_value = tuple(src_row[idx] for idx in pk_indices)
        else:
            idx = pk_indices[0]
            pk_value = src_row[idx]

        for col in value_columns:
            src_val = getattr(src_row, col)
            trg_val = getattr(trg_row, col)
            if src_val != trg_val:

                diff_counters[col] += 1
                if len(diff_examples[col]) < examples_count:
                    diff_examples[col].append({'pk': pk_value, 'src_val': src_val, 'trg_val': trg_val })

    # filter out cols without examples
    diff_examples = {k: v for k, v in diff_examples.items() if v}
    if diff_counters:
        values = (np.array(list(diff_counters.values())) / common_keys_cnt) * 100
        max_pct, median_pct = float(values.max()), float(np.median(values))
        metrics['max_pct'] = max_pct
        metrics['median_pct'] = median_pct


    # transform to dataframes
    # 1
    diff_records = []
    for column_name, records in diff_examples.items():
        for record in records:
            transformed_record = {
                'primary_key': record['pk'],
                'column_name': column_name,
                'source_value': record['src_val'],
                'target_value': record['trg_val'],
            }
            diff_records.append(transformed_record)

    df_diff_examples = pd.DataFrame(diff_records)
    # 2
    df_diff_counters = pd.DataFrame(
        list(diff_counters.items()),  # преобразуем в список кортежей
        columns=['column_name', 'mismatch_count']  # переименовываем колонки
        )

    return metrics, df_diff_examples, df_diff_counters


def compare_dataframes(
    source_df: pd.DataFrame,
    target_df: pd.DataFrame,
    key_columns: List[str],
    max_examples: int = DEFAULT_MAX_EXAMPLES
) -> tuple[ComparisonStats, ComparisonDiffDetails]:
    """
    Efficient comparison of two dataframes by primary key when discrepancies ratio quite small,
    to analyze the difference in primary keys values and column values

    Looks like it can be simplified and optimized by
    1) outer merge join + indicator metrics(left_only, right_only, both) or/and
    2) by vectors

    Parameters:
        source_df : pd.DataFrame
            Source dataframe
        target_df : pd.DataFrame
            Target dataframe for comparison
        key_columns : List[str]
            List of primary key columns
        max_examples : int, optional
            Maximum number of discrepancy examples per column

    Returns:
    --------
    Dict with
        1) ComparisonStats Object with comparison statistics
        2) ComparisonDiffDetails Object with additional details, like the examples and per column diff data
    """
    app_logger.info('start')

    # Input data validation
    if source_df.empty and target_df.empty:
        return None, None
    _validate_input_data(source_df, target_df, key_columns)

    # Check for duplicate primary keys and handle them
    source_dup = source_df[source_df.duplicated(subset=key_columns, keep=False)]
    target_dup = target_df[target_df.duplicated(subset=key_columns, keep=False)]

    source_dup_keys = _create_keys_set(source_dup, key_columns) if not source_dup.empty else set()
    target_dup_keys = _create_keys_set(target_dup, key_columns) if not target_dup.empty else set()

    source_dup_keys_examples = format_keys(source_dup_keys, max_examples)
    target_dup_keys_examples = format_keys(target_dup_keys, max_examples)

    # Remove duplicates from both dataframes for clean comparison
    source_clean = source_df.drop_duplicates(subset=key_columns, keep='first')
    target_clean = target_df.drop_duplicates(subset=key_columns, keep='first')

    # Count duplicates for metrics
    source_dup_cnt = len(source_df) - len(source_clean)
    target_dup_cnt = len(target_df) - len(target_clean)

    non_key_columns = compare_dataframes_meta(source_clean, target_clean, key_columns)

    source_clean = source_clean.assign(xflg='src')
    target_clean = target_clean.assign(xflg='trg')

    xor_combined_df = (
        pd.concat([source_clean, target_clean], ignore_index=True)
        .drop_duplicates(subset=key_columns + non_key_columns, keep=False)
        .assign(xcount_pairs=lambda df: df.groupby(key_columns)[key_columns[0]].transform('size'))
    )

    # symmetrical difference between two datasets, sorted
    xor_combined_sorted = xor_combined_df.sort_values(
        by=key_columns + ['xflg'],
        ascending=[False] * len(key_columns) + [True]
    )

    mask = xor_combined_sorted['xcount_pairs'] > 1
    xor_df_multi = xor_combined_sorted[mask]

    mask_source = xor_combined_sorted['xflg'] == 'src'
    mask_target = xor_combined_sorted['xflg'] == 'trg'
    xor_df_source_only = xor_combined_sorted[~mask & mask_source]
    xor_df_target_only = xor_combined_sorted[~mask & mask_target]

    xor_source_only_keys = _create_keys_set(xor_df_source_only, key_columns)
    xor_target_only_keys = _create_keys_set(xor_df_target_only, key_columns)

    xor_common_keys_cnt = int(len(xor_df_multi)/2) if not xor_df_multi.empty else 0
    xor_source_only_keys_cnt = len(xor_source_only_keys)
    xor_target_only_keys_cnt = len(xor_target_only_keys)

    # take n pairs that is why examples x2
    xor_df_multi_example = xor_df_multi.head(max_examples*2).drop(columns=['xcount_pairs']) if not xor_df_multi.empty else pd.DataFrame()

    xor_source_only_keys_examples = format_keys(xor_source_only_keys, max_examples)
    xor_target_only_keys_examples = format_keys(xor_target_only_keys, max_examples)

    # get number of records that present in two datasets based on primary key
    common_keys_cnt = int((len(source_clean) - xor_source_only_keys_cnt + len(target_clean) - xor_target_only_keys_cnt)/2)

    if not common_keys_cnt:
        #Special case when there is no matched primary keys at all
        comparison_stats = ComparisonStats(
        total_source_rows = len(source_df),
        total_target_rows = len(target_df),
        dup_source_rows = source_dup_cnt,
        dup_target_rows = target_dup_cnt,
        only_source_rows = xor_source_only_keys_cnt,
        only_target_rows = xor_target_only_keys_cnt,
        common_pk_rows = 0,
        total_matched_rows= 0,
        #
        dup_source_percentage_rows = 100,
        dup_target_percentage_rows = 100,
        source_only_percentage_rows = 100,
        target_only_percentage_rows = 100,
        total_diff_percentage_rows = 100,
        #
        max_diff_percentage_cols = 100,
        median_diff_percentage_cols =  100,
        #
        final_diff_score = 100,
        final_score = 0
        )

        comparison_diff_detais = ComparisonDiffDetails(
        mismatches_per_column = pd.DataFrame(),
        discrepancies_per_col_examples = pd.DataFrame(),
        dup_source_keys_examples = source_dup_keys_examples,
        dup_target_keys_examples = target_dup_keys_examples,
        common_attribute_columns=non_key_columns,
        source_only_keys_examples = xor_source_only_keys_examples,
        target_only_keys_examples = xor_target_only_keys_examples,
        discrepant_data_examples = pd.DataFrame())
        app_logger.info('end')

        return comparison_stats, comparison_diff_detais

    # get number of that totally equal in two datasets
    total_matched_records_cnt = common_keys_cnt - xor_common_keys_cnt

    source_only_percentage = (xor_source_only_keys_cnt/common_keys_cnt)*100
    target_only_percentage = (xor_target_only_keys_cnt/common_keys_cnt)*100

    source_dup_percentage = (source_dup_cnt/len(source_df))*100
    target_dup_percentage = (target_dup_cnt/len(target_df))*100

    diff_col_metrics, \
    diff_col_examples,\
    diff_col_counters  = analyze_column_discrepancies(xor_df_multi, key_columns, non_key_columns, common_keys_cnt, max_examples)


    source_and_target_total_diff_percentage = (1-total_matched_records_cnt/common_keys_cnt)*100

    final_diff_score = source_dup_percentage*0.1 + target_dup_percentage*0.1 + \
                       source_only_percentage*0.15 + target_only_percentage*0.15 + \
                       source_and_target_total_diff_percentage*0.5

    comparison_stats = ComparisonStats(
        total_source_rows = len(source_df),
        total_target_rows = len(target_df),
        dup_source_rows = source_dup_cnt,
        dup_target_rows = target_dup_cnt,
        only_source_rows = xor_source_only_keys_cnt,
        only_target_rows = xor_target_only_keys_cnt,
        common_pk_rows = common_keys_cnt,
        total_matched_rows= total_matched_records_cnt,
        #
        dup_source_percentage_rows = source_dup_percentage,
        dup_target_percentage_rows = target_dup_percentage,
        source_only_percentage_rows = source_only_percentage,
        target_only_percentage_rows = target_only_percentage,
        total_diff_percentage_rows = source_and_target_total_diff_percentage,
        #
        max_diff_percentage_cols = diff_col_metrics['max_pct'],
        median_diff_percentage_cols =  diff_col_metrics['median_pct'],
        #
        final_diff_score = final_diff_score,
        final_score = 100 - final_diff_score
        )

    comparison_diff_detais = ComparisonDiffDetails(
        mismatches_per_column = diff_col_counters,
        discrepancies_per_col_examples = diff_col_examples,
        dup_source_keys_examples = source_dup_keys_examples,
        dup_target_keys_examples = target_dup_keys_examples,
        source_only_keys_examples = xor_source_only_keys_examples,
        target_only_keys_examples = xor_target_only_keys_examples,
        discrepant_data_examples = xor_df_multi_example,
        common_attribute_columns=non_key_columns)

    app_logger.info('end')
    return comparison_stats, comparison_diff_detais


def _validate_input_data(
    source_df: pd.DataFrame,
    target_df: pd.DataFrame,
    key_columns: List[str]
) -> None:
    """Input data validation"""
    if not all(col in source_df.columns for col in key_columns):
        missing = [col for col in key_columns if col not in source_df.columns]
        raise ValueError(f"Key columns missing in source: {missing}")

    if not all(col in target_df.columns for col in key_columns):
        missing = [col for col in key_columns if col not in target_df.columns]
        raise ValueError(f"Key columns missing in target: {missing}")


def _create_keys_set(df: pd.DataFrame, key_columns: List[str]) -> set:
    """Creates key set for fast comparison"""
    return set(df[key_columns].itertuples(index=False, name=None))


def generate_comparison_sample_report(source_table:str,
                                   target_table:str,
                                   stats: ComparisonStats,
                                   details: ComparisonDiffDetails,
                                   timezone: str,
                                   source_query: str = None,
                                   source_params: Dict = None,
                                   target_query: str = None,
                                   target_params: Dict = None) -> None:
    """Generate comparison report (logger output looks uuugly)"""
    rl = []
    rl.append("=" * 80)
    current_datetime = datetime.now()
    rl.append(current_datetime.strftime(DATETIME_FORMAT))
    rl.append(f"DATA SAMPLE COMPARISON REPORT: ")
    if source_table and target_table: #empty for custom query
        rl.append(f"{source_table}")
        rl.append(f"VS")
        rl.append(f"{target_table}")
        rl.append("=" * 80)

    if source_query and target_query:
        rl.append(f"timezone: {timezone}")
        rl.append(f"    {source_query}")
        if source_params:
            rl.append(f"    params: {source_params}")
        rl.append("-" * 40)
        rl.append(f"    {target_query}")
        if target_params:
            rl.append(f"    params: {target_params}")

    rl.append("-" * 40)

    rl.append(f"\nSUMMARY:")
    rl.append(f"  Source rows: {stats.total_source_rows}")
    rl.append(f"  Target rows: {stats.total_target_rows}")
    rl.append(f"  Duplicated source rows: {stats.dup_source_rows}")
    rl.append(f"  Duplicated target rows: {stats.dup_target_rows}")
    rl.append(f"  Only source rows: {stats.only_source_rows}")
    rl.append(f"  Only target rows: {stats.only_target_rows}")
    rl.append(f"  Common rows (by primary key): {stats.common_pk_rows}")
    rl.append(f"  Totally matched rows: {stats.total_matched_rows}")
    rl.append("-"*40)
    rl.append(f"  Source only rows %: {stats.source_only_percentage_rows:.5f}")
    rl.append(f"  Target only rows %: {stats.target_only_percentage_rows:.5f}")
    rl.append(f"  Duplicated source rows %: {stats.dup_source_percentage_rows:.5f}")
    rl.append(f"  Duplicated target rows %: {stats.dup_target_percentage_rows:.5f}")
    rl.append(f"  Mismatched rows %: {stats.total_diff_percentage_rows:.5f}")
    rl.append(f"  Final discrepancies score: {stats.final_diff_score:.5f}")
    rl.append(f"  Final data quality score: {stats.final_score:.5f}")


    rl.append(f"  Source-only key examples: {details.source_only_keys_examples}")
    rl.append(f"  Target-only key examples: {details.target_only_keys_examples}")

    rl.append(f"  Duplicated source key examples: {details.dup_source_keys_examples}")
    rl.append(f"  Duplicated target key examples: {details.dup_target_keys_examples}")

    rl.append(f"  Common attribute columns: {', '.join(details.common_attribute_columns)}")
    rl.append(f"  Skipped source columns: {', '.join(details.skipped_source_columns)}")
    rl.append(f"  Skipped target columns: {', '.join(details.skipped_target_columns)}")

    if stats.max_diff_percentage_cols > 0 and not details.mismatches_per_column.empty:
        rl.append(f"\nCOLUMN DIFFERENCES:")

        rl.append(f"  Discrepancies per column (max %): {stats.max_diff_percentage_cols:.5f}")
        rl.append(f"  Count of mismatches per column:\n")
        rl.append(details.mismatches_per_column.to_string(index=False))

        rl.append(f"  Some examples:\n")
        rl.append (details.discrepancies_per_col_examples.to_string(index=False, max_colwidth=64,justify='left'))


    # Display sample data if available
    if details.discrepant_data_examples is not None and not details.discrepant_data_examples.empty:
        rl.append(f"\nDISCREPANT DATA (first pairs):")
        rl.append("Sorted by primary key and dataset:")
        rl.append(f"\n")
        rl.append(details.discrepant_data_examples.to_string(index=False, max_colwidth=64,justify='left'))
        rl.append(f"\n")

    rl.append("=" * 80)

    return "\n".join(rl)

def generate_comparison_count_report(source_table:str,
                                  target_table:str,
                                  stats: ComparisonStats,
                                  details: ComparisonDiffDetails,
                                  total_source_count:int,
                                  total_target_count:int,
                                  discrepancies_counters_percentage:int,
                                  result_diff_in_counters:int,
                                  result_equal_in_counters:int,
                                  timezone: str,
                                  source_query: str = None,
                                  source_params: Dict = None,
                                  target_query: str = None,
                                  target_params: Dict = None) -> None:

    """Generates comparison report (logger output looks uuugly)"""
    rl = []
    rl.append("=" * 80)
    current_datetime = datetime.now()
    rl.append(current_datetime.strftime(DATETIME_FORMAT))
    rl.append(f"COUNT COMPARISON REPORT:")
    rl.append(f"{source_table}")
    rl.append(f"VS")
    rl.append(f"{target_table}")
    rl.append("=" * 80)

    if source_query and target_query:
        rl.append(f"timezone: {timezone}")
        rl.append(f"    {source_query}")
        if source_params:
            rl.append(f"    params: {source_params}")
        rl.append("-" * 40)
        rl.append(f"    {target_query}")
        if target_params:
            rl.append(f"    params: {target_params}")
    rl.append("-" * 40)

    rl.append(f"\nSUMMARY:")
    rl.append(f"  Source total count: {total_source_count}")
    rl.append(f"  Target total count: {total_target_count}")
    rl.append(f"  Common total count: {result_equal_in_counters}")
    rl.append(f"  Diff total count: {result_diff_in_counters}")
    rl.append(f"  Discrepancies percentage: {discrepancies_counters_percentage:.5f}%")
    rl.append(f"  Final discrepancies score: {discrepancies_counters_percentage:.5f}")
    rl.append(f"  Final data quality score: {(100-discrepancies_counters_percentage):.5f}")
    if not details.mismatches_per_column.empty :


        rl.append(f"\nDETAIL DIFFERENCES:")
        rl.append (details.mismatches_per_column.to_string(index=False)

               )

    # Display sample data if available
    if details.discrepant_data_examples is not None and not details.discrepant_data_examples.empty:
        rl.append(f"\nDISCREPANT DATA (first pairs):")
        rl.append("Sorted by primary key and dataset:")
        rl.append(f"\n")
        rl.append(details.discrepant_data_examples.to_string(index=False))
        rl.append(f"\n")
    rl.append("=" * 80)

    return "\n".join(rl)

def safe_remove_zeros(x):
    if pd.isna(x):
        return x
    elif isinstance(x, float) and x.is_integer():
        return int(x)
    return x

def prepare_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Prepare DataFrame for comparison by handling nulls and empty strings"""
    df = df.map(safe_remove_zeros)


    df = df.fillna(NULL_REPLACEMENT)
    df = df.replace(r'(?i)^(None|nan|NaN|\s*)$', NULL_REPLACEMENT, regex=True)

    df = df.astype(str)

    return df

def exclude_by_keys(df, key_columns, exclude_set):
    if len(key_columns) == 1:
        exclude_values = [x[0] for x in exclude_set]
        return df[~df[key_columns[0]].isin(exclude_values)]
    else:
        return df[~df.apply(lambda row: tuple(row[col] for col in key_columns) in exclude_set, axis=1)]


def clean_recently_changed_data(df1:pd.DataFrame, df2:pd.DataFrame, primary_keys:List[str]):
    """
    Mutually removes rows with recently changed records

    Parameters:
        df1, df2: pandas.DataFrame
        primary_keys: list 

    Returns:
        tuple: (df1_processed, df2_processed)
    """
    app_logger.info(f'before exclusion recently changed rows source: {len(df1)}, target {len(df2)}')

    filtered_df1 = df1.copy()
    filtered_df2 = df2.copy()

    filtered_df1 = filtered_df1.loc[filtered_df1['xrecently_changed'] == 'y']
    filtered_df2 = filtered_df2.loc[filtered_df2['xrecently_changed'] == 'y']

    excluded_from_df1_keys = _create_keys_set(filtered_df1,primary_keys)
    excluded_from_df2_keys = _create_keys_set(filtered_df2,primary_keys)

    excluded_keys = excluded_from_df1_keys | excluded_from_df2_keys
    df1_processed = exclude_by_keys(df1, primary_keys, excluded_keys).drop('xrecently_changed', axis=1)
    df2_processed = exclude_by_keys(df2, primary_keys, excluded_keys).drop('xrecently_changed', axis=1)

    app_logger.info(f'after exclusion recently changed rows source: {len(df1_processed)}, target {len(df2_processed)}')

    return df1_processed, df2_processed


def find_count_discrepancies(
    source_counts: pd.DataFrame,
    target_counts: pd.DataFrame
) -> pd.DataFrame:
    """Find discrepancies in daily row counts between source and target"""
    source_counts['flg'] = 'source'
    target_counts['flg'] = 'target'

    # Find mismatches in counts per date
    all_counts = pd.concat([source_counts, target_counts])
    discrepancies = all_counts.drop_duplicates(
        subset=['dt', 'cnt'],
        keep=False
    ).sort_values(
        by=['dt', 'flg'],
        ascending=[False, True]
    )

    return discrepancies

def create_result_message(
    source_total: int,
    target_total: int,
    discrepancies: pd.DataFrame,
    comparison_type: str
) -> str:
    """Create standardized result message"""
    if discrepancies.empty:
        return f"{comparison_type} match: Source={source_total}, Target={target_total}"

    mismatch_count = len(discrepancies)
    diff = source_total - target_total
    diff_msg = f" (Δ={diff})" if diff != 0 else ""

    return (
        f"{comparison_type} mismatch: Source={source_total}, Target={target_total}{diff_msg}, "
        f"{mismatch_count} discrepancies found"
    )

def filter_columns(
    df: pd.DataFrame,
    columns: List[str],
    exclude: Optional[List[str]] = None
) -> pd.DataFrame:
    """Filter DataFrame columns with optional exclusions"""
    if exclude:
        columns = [col for col in columns if col not in exclude]
    return df[columns]

def cross_fill_missing_dates(df1, df2, date_column='dt', value_column='cnt'):
    """
    Fill missing dates between tow dataframes
    """

    df1_indexed = df1.set_index(date_column)
    df2_indexed = df2.set_index(date_column)

    all_dates = df1_indexed.index.union(df2_indexed.index)

    df1_full = df1_indexed.reindex(all_dates, fill_value=0)
    df2_full = df2_indexed.reindex(all_dates, fill_value=0)

    df1_full = df1_full.reset_index()
    df2_full = df2_full.reset_index()

    return df1_full, df2_full

def format_keys(keys, max_examples):
    if keys:
        keys = {next(iter(x)) if len(x) == 1 else x for x in list(keys)[:max_examples]}
        keys = keys if keys != set() else None
        return keys
    else:
        return None

def get_dataframe_size_gb(df: pd.DataFrame) -> float:
    """Calculate DataFrame size in GB"""
    if df.empty:
        return 0.0
    return df.memory_usage(deep=True).sum() / 1024 / 1024 / 1024

def validate_dataframe_size(df: pd.DataFrame, max_size_gb: float) -> None:
    """Validate DataFrame size and raise exception if exceeds limit"""
    if df is None:
        return

    size_gb = get_dataframe_size_gb(df)

    if size_gb > max_size_gb:
        raise ValueError(
            f"DataFrame size {size_gb:.2f} GB exceeds limit of {max_size_gb} GB. "
            f"Shape: {df.shape}"
        )


==================================================
ФАЙЛ: src/xoverrr/exceptions.py
РАЗМЕР: 437 символов
==================================================

class DQCompareException(Exception):
    """Base exception for data quality comparison errors"""
    pass

class MetadataError(DQCompareException):
    """Exception raised for metadata-related errors"""
    pass

class QueryExecutionError(DQCompareException):
    """Exception raised for query execution failures"""
    pass

class TypeConversionError(DQCompareException):
    """Exception raised for type conversion failures"""
    pass


==================================================
ФАЙЛ: src/xoverrr/adapters/clickhouse.py
РАЗМЕР: 7195 символов
==================================================

import pandas as pd
from typing import Optional, Dict, Callable, List, Tuple, Union
from ..constants import DATE_FORMAT, DATETIME_FORMAT
from .base import BaseDatabaseAdapter, Engine
from ..models import DataReference, ObjectType
from ..exceptions import QueryExecutionError
import time
from ..logger import app_logger

class ClickHouseAdapter(BaseDatabaseAdapter):
    """ClickHouse adapter with parameterized queries"""
    def _execute_query(self, query: Union[str, Tuple[str, Dict]], engine: Engine, timezone: str) -> pd.DataFrame:
        df = None
        tz_set = None
        start_time = time.time()
        app_logger.info('start')

        if timezone:
            tz_set = f"SETTINGS session_timezone = '{timezone}'"
        try:
            if isinstance(query, tuple):
                query, params = query
                if tz_set:
                    query = f'{query} {tz_set}'
                app_logger.info(f'query\n {query}')
                app_logger.info(f'{params=}')
                df = pd.read_sql(query, engine, params=params)
            else:
                if tz_set:
                    query = f'{query} {tz_set}'
                app_logger.info(f'query\n {query}')
                df = pd.read_sql(query, engine)

            execution_time = time.time() - start_time
            app_logger.info(f"Query executed in {execution_time:.2f}s")
            return df

        except Exception as e:
            execution_time = time.time() - start_time
            app_logger.error(f"Query execution failed after {execution_time:.2f}s: {str(e)}")

            raise QueryExecutionError(f"Query failed: {str(e)}")

    def get_object_type(self, data_ref: DataReference, engine: Engine) -> ObjectType:
        """Determine if object is table or view in ClickHouse"""
        query = """
            SELECT
                engine as table_engine,
                if(engine = 'View', 'view', 'table') as object_type
            FROM system.tables
            WHERE database = %(schema)s
            AND name = %(table)s
        """
        params = {'schema': data_ref.schema, 'table': data_ref.name}

        try:
            result = self._execute_query((query, params), engine, None)
            if not result.empty:
                type_str = result.iloc[0]['object_type']
                engine_str = result.iloc[0]['table_engine']

                # ClickHouse имеет разные типы таблиц
                if engine_str == 'View':
                    return ObjectType.VIEW
                elif engine_str in ['MaterializedView', 'MaterializeView']:
                    return ObjectType.MATERIALIZED_VIEW
                else:
                    return ObjectType.TABLE
        except Exception as e:
            app_logger.warning(f"Could not determine object type for {data_ref.full_name}: {str(e)}")

        return ObjectType.UNKNOWN

    def build_metadata_columns_query(self, data_ref: DataReference) -> Tuple[str, Dict]:
        query = """
            SELECT
                name as column_name,
                type as data_type,
                position as column_id
            FROM system.columns
            WHERE database = %(schema)s
            AND table = %(table)s
            ORDER BY position
        """
        params = {'schema': data_ref.schema, 'table': data_ref.name}
        return query, params

    def build_primary_key_query(self, data_ref: DataReference) -> Tuple[str, Dict]:
        query = """
            SELECT name as pk_column_name
            FROM system.columns
            WHERE database = %(schema)s
            AND table = %(table)s
            AND is_in_primary_key = 1
            ORDER BY position
        """
        params = {'schema': data_ref.schema, 'table': data_ref.name}
        return query, params

    def build_count_query(self, data_ref: DataReference, date_column: str,
                         start_date: Optional[str], end_date: Optional[str]) -> Tuple[str, Dict]:
        query = f"""
            SELECT
                formatDateTime(toDate({date_column}), '%%Y-%%m-%%d') as dt,
                count(*) as cnt
            FROM {data_ref.full_name}
            WHERE 1=1
        """
        params = {}


        if start_date:
            query += f" AND {date_column} >= toDate(%(start_date)s)"
            params['start_date'] = start_date
        if end_date:
            query += f" AND {date_column} < toDate(%(end_date)s) + INTERVAL 1 day"
            params['end_date'] = end_date

        query += " GROUP BY dt ORDER BY dt DESC"
        return query, params

    def build_data_query(self, data_ref: DataReference, columns: List[str],
                        date_column: Optional[str], update_column: str,
                        start_date: Optional[str], end_date: Optional[str],
                        exclude_recent_hours: Optional[int] = None) -> Tuple[str, Dict]:
        params = {}
        # Add recent data exclusion flag
        exclusion_condition,  exclusion_params = self._build_exclusion_condition(
            update_column, exclude_recent_hours
        )

        if exclusion_condition:
            columns.append(exclusion_condition)
            params.update(exclusion_params)

        query = f"""
        SELECT {', '.join(columns)}
        FROM {data_ref.full_name}
        WHERE 1=1\n"""

        if start_date and date_column:
            query += f"            AND {date_column} >= toDate(%(start_date)s)\n"
            params['start_date'] = start_date
        if end_date and date_column:
            query += f"            AND {date_column} < toDate(%(end_date)s) + INTERVAL 1 day\n"
            params['end_date'] = end_date

        return query, params

    def _build_exclusion_condition(self, update_column: str,
                                    exclude_recent_hours: int) -> Tuple[str, Dict]:
        """ClickHouse-specific implementation for recent data exclusion"""
        if  update_column and exclude_recent_hours:


            exclude_recent_hours = exclude_recent_hours

            condition = f"""case when {update_column} > (now() - INTERVAL %(exclude_recent_hours)s HOUR) then 'y' end as xrecently_changed"""
            params = {'exclude_recent_hours':  exclude_recent_hours}
            return condition, params

        return None, None

    def _get_type_conversion_rules(self, timezone:str ) -> Dict[str, Callable]:
        return {
            r'datetime\(': lambda x: pd.to_datetime(x, utc=True, errors='coerce').dt.tz_convert(timezone).dt.tz_localize(None).strftime(DATETIME_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'datetime64': lambda x: pd.to_datetime(x, utc=True, errors='coerce').dt.tz_convert(timezone).dt.tz_localize(None).strftime(DATETIME_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'datetime': lambda x: pd.to_datetime(x, errors='coerce').dt.strftime(DATETIME_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'date': lambda x: pd.to_datetime(x, errors='coerce').dt.strftime(DATE_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'uint64|uint8|float|decimal|int32': lambda x: x.astype(str).str.replace(r'\.0+$', '', regex=True),
        }


==================================================
ФАЙЛ: src/xoverrr/adapters/__init__.py
РАЗМЕР: 243 символов
==================================================

from .base import BaseDatabaseAdapter
from .oracle import OracleAdapter
from .postgres import PostgresAdapter
from .clickhouse import ClickHouseAdapter

__all__ = ['BaseDatabaseAdapter', 'OracleAdapter', 'PostgresAdapter', 'ClickHouseAdapter']


==================================================
ФАЙЛ: src/xoverrr/adapters/postgres.py
РАЗМЕР: 7888 символов
==================================================

import pandas as pd
from typing import Optional, Dict, Callable, List, Tuple, Union
from ..constants import DATETIME_FORMAT
from .base import BaseDatabaseAdapter, Engine
from ..models import DataReference, ObjectType
from ..exceptions import QueryExecutionError
from json import dumps

from ..logger import app_logger
import time

class PostgresAdapter(BaseDatabaseAdapter):


    def _execute_query(self, query: Union[str, Tuple[str, Dict]], engine: Engine, timezone: str) -> pd.DataFrame:

        df = None
        tz_set = None
        start_time = time.time()
        app_logger.info('start')

        if timezone:
            tz_set = f"set time zone '{timezone}';"

        try:
            if isinstance(query, tuple):
                query, params = query
                if tz_set:
                    query = f'{tz_set}\n{query}'
                app_logger.info(f'query\n {query}')
                app_logger.info(f'{params=}')
                df = pd.read_sql(query, engine, params=params)
            else:
                if tz_set:
                    query = f'{tz_set}\n{query}'
                app_logger.info(f'query\n {query}')
                df = pd.read_sql(query, engine)
            execution_time = time.time() - start_time
            app_logger.info(f"Query executed in {execution_time:.2f}s")
            app_logger.info('complete')
            return df
        except Exception as e:
            execution_time = time.time() - start_time
            app_logger.error(f"Query execution failed after {execution_time:.2f}s: {str(e)}")
            raise QueryExecutionError(f"Query failed: {str(e)}")


    def get_object_type(self, data_ref: DataReference, engine: Engine) -> ObjectType:
        """Determine if object is table, view, or materialized view"""
        query = """
            SELECT
                CASE
                    WHEN relkind = 'r' THEN 'table'
                    WHEN relkind = 'v' THEN 'view'
                    WHEN relkind = 'm' THEN 'materialized_view'
                    ELSE 'unknown'
                END as object_type
            FROM pg_class c
            JOIN pg_namespace n ON n.oid = c.relnamespace
            WHERE n.nspname = %(schema)s
            AND c.relname = %(table)s
        """
        params = {'schema': data_ref.schema, 'table': data_ref.name}

        try:
            result = self._execute_query((query, params), engine, None)
            if not result.empty:
                type_str = result.iloc[0]['object_type']
                return {
                    'table': ObjectType.TABLE,
                    'view': ObjectType.VIEW,
                    'materialized_view': ObjectType.MATERIALIZED_VIEW
                }.get(type_str, ObjectType.UNKNOWN)
        except Exception as e:
            app_logger.warning(f"Could not determine object type for {data_ref.full_name}: {str(e)}")

        return ObjectType.UNKNOWN

    def build_metadata_columns_query(self, data_ref: DataReference) -> pd.DataFrame:

        query = """
            SELECT
                lower(column_name) as column_name,
                lower(data_type) as data_type,
                ordinal_position as column_id
            FROM information_schema.columns
            WHERE table_schema = %(schema)s
            AND table_name = %(table)s
            ORDER BY ordinal_position
        """
        params = {'schema': data_ref.schema, 'table': data_ref.name}
        return query, params

    def build_primary_key_query(self, data_ref: DataReference) -> pd.DataFrame:
        """Build primary key query with GreenPlum compatibility"""
        query = """
            select
                pg_attribute.attname as pk_column_name
            from pg_index
            join pg_class on pg_class.oid = pg_index.indrelid
            join pg_attribute on pg_attribute.attrelid = pg_class.oid
                            and pg_attribute.attnum = any(pg_index.indkey)
            join pg_namespace on pg_namespace.oid = pg_class.relnamespace
            where pg_namespace.nspname = %(schema)s
            and pg_class.relname = %(table)s
            and pg_index.indisprimary
            order by pg_attribute.attnum
        """

        params = {'schema': data_ref.schema, 'table': data_ref.name}
        return query, params

    def build_count_query(self, data_ref: DataReference, date_column: str,
                          start_date: Optional[str], end_date: Optional[str]
                         ) -> Tuple[str, Dict]:
        query = f"""
            SELECT
                to_char(date_trunc('day', {date_column}),'YYYY-MM-DD') as dt,
                count(*) as cnt
            FROM {data_ref.full_name}
            WHERE 1=1\n"""
        params = {}

        if start_date:
            query += f" AND {date_column} >= date_trunc('day', %(start_date)s::date)\n"
            params['start_date'] = start_date
        if end_date:
            query += f" AND {date_column} < date_trunc('day', %(end_date)s::date)  + interval '1 days'\n"
            params['end_date'] = end_date

        query += f" GROUP BY to_char(date_trunc('day', {date_column}),'YYYY-MM-DD') ORDER BY dt DESC"
        return query, params

    def build_data_query(self, data_ref: DataReference, columns: List[str],
                        date_column: Optional[str], update_column: str,
                        start_date: Optional[str], end_date: Optional[str],
                        exclude_recent_hours: Optional[int] = None) -> Tuple[str, Dict]:

        params = {}
        # Add recent data exclusion flag
        exclusion_condition,  exclusion_params = self._build_exclusion_condition(
            update_column, exclude_recent_hours
        )

        if exclusion_condition:
            columns.append(exclusion_condition)
            params.update(exclusion_params)

        query = f"""
        SELECT {', '.join(columns)}
        FROM {data_ref.full_name}
        WHERE 1=1\n"""

        if start_date and date_column:
            query += f"            AND {date_column} >= date_trunc('day', %(start_date)s::date)\n"
            params['start_date'] = start_date
        if end_date and date_column:
            query += f"            AND {date_column} < date_trunc('day', %(end_date)s::date)  + interval '1 days'\n"
            params['end_date'] = end_date

        return query, params

    def _build_exclusion_condition(self, update_column: str,
                                    exclude_recent_hours: int) -> Tuple[str, Dict]:
        """PostgreSQL-specific implementation for recent data exclusion"""
        if  update_column and exclude_recent_hours:


            exclude_recent_hours = exclude_recent_hours

            condition = f"""case when {update_column} > (now() - INTERVAL '%(exclude_recent_hours)s hours') then 'y' end as xrecently_changed"""
            params = {'exclude_recent_hours':  exclude_recent_hours}
            return condition, params

        return None, None

    def _get_type_conversion_rules(self, timezone) -> Dict[str, Callable]:
        return {
            r'date': lambda x: pd.to_datetime(x, errors='coerce').dt.strftime(DATETIME_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'boolean': lambda x: x.map({True: '1', False: '0', None: ''}),
            r'timestamptz|timestamp.*\bwith\b.*time\szone': lambda x: pd.to_datetime(x, utc=True, errors='coerce').dt.tz_convert(timezone).dt.tz_localize(None).dt.strftime(DATETIME_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'timestamp': lambda x: pd.to_datetime(x, errors='coerce').dt.strftime(DATETIME_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'integer|numeric|double|float|double precision|real': lambda x: x.astype(str).str.replace(r'\.0+$', '', regex=True),
            r'json': lambda x: '"' + x.astype(str).str.replace(r'"', '\\"', regex=True) + '"',
        }


==================================================
ФАЙЛ: src/xoverrr/adapters/oracle.py
РАЗМЕР: 8605 символов
==================================================

import pandas as pd
from typing import Optional, Dict, Callable, List, Tuple, Union

from ..constants import DATETIME_FORMAT
from .base import BaseDatabaseAdapter, Engine
from ..models import DataReference, ObjectType
from ..exceptions import QueryExecutionError
from ..logger import app_logger
import time

class OracleAdapter(BaseDatabaseAdapter):

    def _execute_query(self, query: Union[str, Tuple[str, Dict]], engine: Engine, timezone: str) -> pd.DataFrame:
        tz_set = None
        raw_conn = None
        cursor = None

        start_time = time.time()
        app_logger.info('start')

        if timezone:
            tz_set = f"alter session set time_zone = '{timezone}'"

        try:
            raw_conn = engine.raw_connection()
            cursor = raw_conn.cursor()

            if tz_set:
                app_logger.info(f'{tz_set}')
                cursor.execute(tz_set)

            cursor.arraysize = 100000

            if isinstance(query, tuple):
                query_text, params = query
                app_logger.info(f'query\n {query_text}')
                app_logger.info(f'{params=}')
                cursor.execute(query_text, params or {})
            else:
                app_logger.info(f'query\n {query}')
                cursor.execute(query)


            columns = [col[0].lower() for col in cursor.description]
            data = cursor.fetchall()

            execution_time = time.time() - start_time
            app_logger.info(f"Query executed in {execution_time:.2f}s")

            app_logger.info('complete')

            # excplicitly close cursor before closing the connection
            if cursor:
                cursor.close()

            return pd.DataFrame(data, columns=columns)

        except Exception as e:
            execution_time = time.time() - start_time
            app_logger.error(f"Query execution failed after {execution_time:.2f}s: {str(e)}")

            if raw_conn:
                try:
                    raw_conn.rollback()
                except Exception as rollback_error:
                    app_logger.warning(f"Rollback failed: {rollback_error}")
                try:
                    if cursor:
                        cursor.close()
                except Exception as close_error:
                    app_logger.warning(f"Cursor close failed: {close_error}")

            raise QueryExecutionError(f"Query failed: {str(e)}")

    def get_object_type(self, data_ref: DataReference, engine: Engine) -> ObjectType:
        """Determine if object is table or view in Oracle"""
        query = """
            SELECT
                CASE
                    WHEN object_type = 'TABLE' THEN 'table'
                    WHEN object_type = 'VIEW' THEN 'view'
                    WHEN object_type = 'MATERIALIZED VIEW' THEN 'materialized_view'
                    ELSE 'unknown'
                END as object_type
            FROM all_objects
            WHERE owner = UPPER(:schema_name)
            AND object_name = UPPER(:table_name)
        """
        params = {'schema_name': data_ref.schema, 'table_name': data_ref.name}

        try:
            result = self._execute_query((query, params), engine, None)
            if not result.empty:
                type_str = result.iloc[0]['object_type']
                return {
                    'table': ObjectType.TABLE,
                    'view': ObjectType.VIEW,
                    'materialized_view': ObjectType.MATERIALIZED_VIEW
                }.get(type_str, ObjectType.UNKNOWN)
        except Exception as e:
            app_logger.warning(f"Could not determine object type for {data_ref.full_name}: {str(e)}")

        return ObjectType.UNKNOWN

    def build_metadata_columns_query(self, data_ref: DataReference) -> pd.DataFrame:
        query = """
            SELECT
                lower(column_name) as column_name,
                lower(data_type) as data_type,
                column_id
            FROM all_tab_columns
            WHERE owner = upper(:schema_name)
            AND table_name = upper(:table_name)
            ORDER BY column_id
        """
        params = {}

        params['schema_name'] = data_ref.schema
        params['table_name'] = data_ref.name
        return query, params

    def build_primary_key_query(self, data_ref: DataReference) -> pd.DataFrame:

        #todo add suport of unique indexes when no pk?
        query = """
            SELECT lower(cols.column_name) as pk_column_name
            FROM all_constraints cons
            JOIN all_cons_columns cols ON
                cols.owner = cons.owner AND
                cols.table_name = cons.table_name AND
                cols.constraint_name = cons.constraint_name
            WHERE cons.constraint_type = 'P'
            AND cons.owner = upper(:schema_name)
            AND cons.table_name = upper(:table_name)
        """
        params = {}

        params['schema_name'] = data_ref.schema
        params['table_name'] = data_ref.name
        return query, params


    def build_count_query(self, data_ref: DataReference, date_column: str,
                            start_date: Optional[str], end_date: Optional[str]) -> Tuple[str, Dict]:
        query = f"""
            SELECT
                to_char(trunc({date_column}, 'dd'),'YYYY-MM-DD') as dt,
                count(*) as cnt
            FROM {data_ref.full_name}
            WHERE 1=1\n"""
        params = {}


        if start_date:
            query += f" AND {date_column} >= trunc(to_date(:start_date, 'YYYY-MM-DD'), 'dd')\n"
            params['start_date'] = start_date
        if end_date:
            query += f" AND {date_column} < trunc(to_date(:end_date, 'YYYY-MM-DD'), 'dd') + 1\n"
            params['end_date'] = end_date

        query += f" GROUP BY to_char(trunc({date_column}, 'dd'),'YYYY-MM-DD') ORDER BY dt DESC"
        return query, params

    def build_data_query(self, data_ref: DataReference, columns: List[str],
                        date_column: Optional[str], update_column: str,
                        start_date: Optional[str], end_date: Optional[str],
                        exclude_recent_hours: Optional[int] = None) -> Tuple[str, Dict]:

        params = {}
        # Add recent data exclusion flag
        exclusion_condition,  exclusion_params = self._build_exclusion_condition(
            update_column, exclude_recent_hours
        )

        if exclusion_condition:
            columns.append(exclusion_condition)
            params.update(exclusion_params)

        query = f"""
        SELECT {', '.join(columns)}
        FROM {data_ref.full_name}
        WHERE 1=1\n"""

        if start_date and date_column:
            query += f"            AND {date_column} >= trunc(to_date(:start_date, 'YYYY-MM-DD'), 'dd')\n"
            params['start_date'] = start_date

        if end_date and date_column:
            query += f"            AND {date_column} < trunc(to_date(:end_date, 'YYYY-MM-DD'), 'dd') + 1\n"
            params['end_date'] = end_date

        return query, params

    def _build_exclusion_condition(self, update_column: str,
                                    exclude_recent_hours: int) -> Tuple[str, Dict]:
        """Oracle-specific implementation for recent data exclusion"""
        if  update_column and exclude_recent_hours:



            condition = f"""case when {update_column} > (sysdate - :exclude_recent_hours/24) then 'y' end as xrecently_changed"""
            params = {'exclude_recent_hours':  exclude_recent_hours}
            return condition, params

        return None, None

    def _get_type_conversion_rules(self, timezone: str) -> Dict[str, Callable]:
        return {
            #errors='coerce' is needed as workaround for >= 2262 year: Out of bounds nanosecond timestamp (3023-04-04 00:00:00)
            #  todo need specify explicit dateformat (nls params) in sessions, for the correct string conversion to datetime
            r'date': lambda x: pd.to_datetime(x, errors='coerce').dt.strftime(DATETIME_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'timestamp.*\bwith\b.*time\szone': lambda x: pd.to_datetime(x, utc=True, errors='coerce').dt.tz_convert(timezone).dt.tz_localize(None).dt.strftime(DATETIME_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'timestamp': lambda x: pd.to_datetime(x, errors='coerce').dt.strftime(DATETIME_FORMAT).str.replace(r'\s00:00:00$', '', regex=True),
            r'number|float|double': lambda x: x.astype(str).str.replace(r'\.0+$', '', regex=True).str.lower(), #lower case for exponential form compare
        }



==================================================
ФАЙЛ: src/xoverrr/adapters/base.py
РАЗМЕР: 4934 символов
==================================================

from abc import ABC, abstractmethod
import pandas as pd
from typing import Dict, Callable, List, Tuple, Optional, Union
import re
from datetime import datetime, timedelta
from ..models import DataReference, ObjectType
from ..constants import RESERVED_WORDS
from sqlalchemy.engine import Engine
from ..logger import app_logger
from ..logger import app_logger

class BaseDatabaseAdapter(ABC):
    """Abstract base class with updated method signatures for parameterized queries"""
    @abstractmethod
    def _execute_query(self, query: Union[str, Tuple[str, Dict]], engine: Engine, timezone:str) -> pd.DataFrame:
        """Execute query with DBMS-specific optimizations"""
        pass

    @abstractmethod
    def get_object_type(self, data_ref: DataReference, engine: Engine) -> ObjectType:
        """Determine database object type"""
        pass

    @abstractmethod
    def build_metadata_columns_query(self, data_ref: DataReference) -> Tuple[str, Dict]:
        pass

    @abstractmethod
    def build_primary_key_query(self, data_ref: DataReference) -> Tuple[str, Dict]:
        pass

    @abstractmethod
    def build_count_query(self, data_ref: DataReference, date_column: str,
                         start_date: Optional[str], end_date: Optional[str]
                         ) -> Tuple[str, Dict]:
        """Returns tuple of (query, params) with recent data exclusion"""
        pass

    def build_data_query_common(self, data_ref: DataReference, columns: List[str],
                        date_column: Optional[str], update_column: Optional[str],
                        start_date: Optional[str], end_date: Optional[str],
                        exclude_recent_hours: Optional[int] = None) -> Tuple[str, Dict]:
        """Build data query for the DBMS with recent data exclusion"""
        # Handle reserved words
        cols_select = [
            f'"{col}"' if col.lower() in RESERVED_WORDS
            else col
            for col in columns
        ]

        result = self.build_data_query(data_ref, cols_select, date_column, update_column,
                                     start_date, end_date, exclude_recent_hours)
        return result

    @abstractmethod
    def build_data_query(self, data_ref: DataReference, columns: List[str],
                        date_column: Optional[str], update_column: Optional[str],
                        start_date: Optional[str], end_date: Optional[str],
                        exclude_recent_hours: Optional[int] = None) -> Tuple[str, Dict]:
        pass

    @abstractmethod
    def _build_exclusion_condition(self, update_column: str,
                                 exclude_recent_hours: int) -> Tuple[str, Dict]:
        """DBMS-specific implementation for recent data exclusion"""
        pass

    def convert_types(self, df: pd.DataFrame, metadata: pd.DataFrame, timezone: str) -> pd.DataFrame:
        """Convert DBMS-specific types to standardized formats"""
        # there is need to specify timezone for covnersion as
        #   pandas implicitly converts to UTC tz aware cols
        #   and there is general way for different version of pandas to disable this
        type_rules = self._get_type_conversion_rules(timezone)
        return self._apply_type_conversion(df, metadata, type_rules)

    @abstractmethod
    def _get_type_conversion_rules(self, timezone: str) -> Dict[str, Callable]:
        """Get type conversion rules for specific DBMS"""
        pass

    def _apply_type_conversion(self, df: pd.DataFrame, metadata: pd.DataFrame,
                             type_rules: Dict[str, Callable]) -> pd.DataFrame:
        """Apply type conversion rules to DataFrame"""
        if df.empty:
            return df

        app_logger.debug(f'rules: {type_rules.items()}')
        app_logger.debug(f'df.dtypes: {df.dtypes}')
        app_logger.debug(f'db col metadata: {metadata}')

        # apply conversion based on db col meta only
        for _, col_info in metadata.iterrows():
            col_name = col_info['column_name']
            if col_name not in df.columns:
                continue


            col_type = col_info['data_type'].lower()
            # Find matching conversion rule
            converter = None
            for pattern, rule in type_rules.items():
                if re.search(pattern, col_type):
                    converter = rule
                    app_logger.debug(f'{col_name=}: found rule {converter=}')
                    break

            if converter is None:
                continue # Skip columns without converters

            try:
                df[col_name] = converter(df[col_name])
            except Exception as e:
                app_logger.warning(f"Type conversion failed for {col_name}: {str(e)}")
                df[col_name] = df[col_name].astype(str)

            new_type = df[col_name].dtype
            app_logger.debug(f'old: {col_type}, new: {new_type}')

        return df

